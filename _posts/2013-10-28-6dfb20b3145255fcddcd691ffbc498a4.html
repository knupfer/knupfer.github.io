---
layout: post
title: Adaptives Testen
---
<div class="outline-text-2" id="text-2">
<p>
<span class="timestamp-wrapper"><span class="timestamp">[2013-10-28 Mon]</span></span>
</p>
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Einleitung</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Beim adaptiven Testen werden die Möglichkeiten einer computerbasierten
Testung in erweitertem Maße ausgenutzt.  Es wird innerhalb der Testung
auf Grund der bereits beantworteten Fragen auf den Fähigkeitsgrad des
Probanden geschätzt, um ihm als nächstes eine Frage zu geben, die
diesen reflektiert.
</p>


<div class="figure">
<p><img src="/images/adaptiveditaa.png" alt="adaptiveditaa.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Kriterien</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Wie genau die Frage ausgewählt wird hängt vornehmlich von den Zielen
der Testung und den Nebenwirkungen, die man ggf. vermeiden möchte, ab.
So wäre eine Frage, die der Proband mit einer Chance von 50% lösen
kann ideal im Sinne eines Informationsgewinnes über den Probanden, da
die Entropie maximal ist.
</p>

<p>
Entropie stellt nicht nur Chaos, sondern auch Informationsdichte nach
Shannon dar (vgl. <a href="http://de.wikipedia.org/wiki/Entropie_(Informationstheorie)">Wikipedia</a>).  Die folgende Formel ergiebt die
Entropie eines Ereignisses. Hierbei ist \(n\) die Anzahl der
Möglichkeiten und \(p_i\) die jeweiligen Wahrscheinlichkeiten dieser
(sich ausschließender) Möglichkeiten.  $$ H = - \sum_{i=1}^n p_i \cdot
\log_2{p_i} $$
</p>

<p>
Betrachten wir ein binäres System, also nur Fragen die entweder
vollständig falsch oder vollständig richtig beantwortet werden können,
so haben wir \(n = 2\) und \(p_2 = 1 - p_1\).  $$ H_2 = - p \cdot \log_2{p} -
(1 - p) \cdot \log_2(1 - p) $$
</p>

<p>
Hieraus ergiebt sich, dass der Informationsgewinn bei \(p = .5\) (im
binären) maximal ist, da die Funktion symmetrisch ist und bei \(p = 0\)
auf beiden Seiten \(0\) ergibt.  $$ H_{max} = - 0.5 \cdot \log_2{0.5} - 0.5
\cdot \log_2{0.5} = 1 \mathrm{bit} $$
</p>

<p>
Hierbei ist \(bit\) die übliche Einheit des Informationsgehaltes, auf
Grund des \(\log_2\). Es kann also ein Informationsgehalt von 8 bit mit
Hilfe eines bytes dargestellt werden, sprich mit 8 Nullen oder Einsen
(z.B. 10101010).
</p>


<div class="figure">
<p><img src="/images/entropie.png" alt="entropie.png" />
</p>
</div>

<p>
Mit der Formel für tatsächliche und für maximale Entropie kann die
Redundanz ausgerechnet werden, welche in einer Testkonstruktion als
Indikator für das Potential der Verbesserung durch ein adaptives
Testverfahren benutzt werden kann.  $$ R = H_{max} - H $$
</p>

<p>
Somit kann im binären ein Test im Idealfall um \(R\) Fragen verkleinert
werden, ohne an Informationen einzubüßen.  Befinden sich im Test auch
Fragen, die nicht binär sind, so verändert sich die Situation ein
wenig.  Die Entropie wird stets maximal bei gleichen
Wahrscheinlichkeiten.  Gibt es nun bei einer Frage z.B. 0, 1, 2 oder 3
Punkte gibt es eine maximale Entropie von \(H_{max} = - \log_2{0.25} =
2 \mathrm{bit}\). Somit kann man einen Test auch um \(R/2\) solcher
Fragen ohne Informationsverlust verkleinern.
</p>
</div>

<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Informationsgehalt in KoMus</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
Im August diesen Jahres habe ich mit Teilen des KoMus-Testes für
musikalische Kompetenz eine empirische Studie durchgeführt.  Der
KoMus-Test liegt in einem nicht adaptiven Format vor.
</p>

<p>
Interessant ist nun, die Überlegung, wieviel der Test von einer
Überführung in einen adaptiven Test profitieren würde.
</p>
</div>

<div id="outline-container-sec-2-2-1-1" class="outline-5">
<h5 id="sec-2-2-1-1"><span class="section-number-5">2.2.1.1</span> Durchschnittlicher Schüler</h5>
<div class="outline-text-5" id="text-2-2-1-1">
<p>
Der Simplizität halber können wir annehmen, dass ein Schüler genau die
durchschnittlichen Lösungswahrscheinlichkeiten für ein Item aufweist.
So muss man nur die Entropie mit den klassischen Itemschwierigkeiten
der Items berechnen: $$ H = \sum_{i=1}^n(- P_i \cdot \log_2{P_i} - (1 - P_i)
\cdot \log_2(1 - P_i)) $$
</p>

<p>
Hierbei ist \(P_i\) die klassische Itemschwierigkeit des Items \(i\) und
\(n\) die Anzahl der Items.  Ferner nimmt diese Formel nur dichotome
Items an.
</p>
</div>
</div>

<div id="outline-container-sec-2-2-1-2" class="outline-5">
<h5 id="sec-2-2-1-2"><span class="section-number-5">2.2.1.2</span> Vierdimensional</h5>
<div class="outline-text-5" id="text-2-2-1-2">
<p>
Die Entropie ist jedoch eigentlich noch niedriger, wenn man die
Schwierigkeiten adaptiv berechnet.  Es handelt sich aber immernoch um
einen nicht adaptiven Test mit fester Reihenfolge. Es wird nur eine
spezialform der Entropie, die bedingte Entropie, benutzt
(vgl. <a href="http://de.wikipedia.org/wiki/Bedingte_Entropie">Wikipedia</a>).
</p>

<p>
Um die bedingte Lösungschance und somit auch die bedingte Entropie zu
berechnen, werden alle Items der selben Dimension einer logistisch
binären Regression verwendet um die individuell Itemschwierigkeit
eines Items der gleichen Dimension zu berechnen.  $$ H =
\sum_{D=1}^4(\sum_{i=1}^{n(D)}(- R \cdot \log_2{R}) - (1 - R) \cdot
\log_2(1- R))$$ $$ R = Reg_2(P_{ivD}|\sum_{m=1}^{i(D)-1}P_{vmD}) $$
</p>

<p>
Hierbei ist \(Reg_2(a|b)\) die binärlogistische Regression mit der AV
\(a\) und den UV \(b\).
</p>
</div>
</div>

<div id="outline-container-sec-2-2-1-3" class="outline-5">
<h5 id="sec-2-2-1-3"><span class="section-number-5">2.2.1.3</span> n-Dimensional</h5>
<div class="outline-text-5" id="text-2-2-1-3">
<p>
Das obige Modell nimmt jedoch an, dass die einzelnen Dimensionen nicht
korrelieren und somit Itemantworten einer Dimension keine Information
(also Entropiesenkung) über andere Dimensionen zulassen.
</p>

<p>
Dies ist aber eine nicht notwendige Einschränkung, die die Berechnung
nicht nur weniger effektiv, sondern auch komplizierter macht.
</p>

<p>
Berücksichtigen wir unabhängig von der Dimensionszugehörigkeit einfach
alle bereits beantworteten Items, die einen signifikanten Einfluss auf
die Frage haben, haben wir im Endeffekt ein n-dimensionales Modell,
wobei \(n\) die Anzahl der Fragen ist.  $$ H = \sum_{i=1}^n(- R \cdot
\log_2{R} - (1 - R) \cdot \log_2(1 - R)) $$ $$ R =
Reg_2(P_{iv}|\sum_{m=1}^{i-1}P_{vm}) $$
</p>
</div>

<ol class="org-ol"><li>Probleme<br  /><div class="outline-text-6" id="text-2-2-1-3-1">
<p>
Mögliche Probleme dieser Methode sind fehlende Datensätze, da die
binärlogistische Regression normalerweise alle Fälle ausschließt, die
auch nur bei einem der UV keinen Wert hat.  Durch den
maßgeschneiderten Testweg ist es bei einem Itempool, der nicht
komplett erschöpft wird, unmöglich diese Regression so durchzuführen.
Dementsprechend müssen andere Methoden gewählt werden, um mit
fehlenden Daten umzugehen.
</p>

<p>
Eine Möglichkeit wäre, nicht vorhandene Antworten in den Datensätzen
zu simulieren.  Diese Simulation würde von den Items die am sichersten
geschätzt werden können zu den Items, die schwer geschätzt werden
können stattfinden.
</p>

<p>
Der Grund hierfür liegt daran, dass bei einer sehr sicheren Schätzung
der Entropiegehalt sich nur wenig ändert, aber es gleich viel mehr
Personen gibt, die seriös für schwerere Schätzungen verwendet werden
können, was diese Schätzungen erleichtert.
</p>

<p>
Die Simulation sollte nach jeder Testung durchgeführt werden, um die
Simulation mehr und mehr zu verbessern.  Das dieses Verfahren direkt
einen Nutzen erbringt, sollte daran sichtbar sein, dass die Lösungen
der alten Datensätze immer besser retrospektiv vorhergesagt werden
können und somit davon außgegangen werden kann, dass auch bei
aktuellen Testungen die Schätzungen besser sind und somit effektiver
gemessen werden kann.
</p>
</div>
</li></ol>
</div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Umsetzung</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Die Umsetzung wurde mit R bewerkstelligt. Hier traten auch schnell
Probleme auf.  So wurde die Rechenzeit bei etwas komplizierteren
Modellen sehr lang, was natürlich auch an meinem Computer liegt.
Nichts desto trotz ergaben sich Situationen, in denen der Computer 5
Tage lang rechnen hätte müssen.
</p>

<p>
In anderen Situationen wurde das komplette RAM des Computers
aufgezehrt usw.
</p>
</div>

<div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> Programmierung</h4>
<div class="outline-text-4" id="text-2-3-1">
</div><div id="outline-container-sec-2-3-1-1" class="outline-5">
<h5 id="sec-2-3-1-1"><span class="section-number-5">2.3.1.1</span> Initialisierung</h5>
<div class="outline-text-5" id="text-2-3-1-1">
<p>
Für alle nachfolgenden Berechnungen habe ich immer dieses Skript
benutzt, um grundlegende Dinge, wie Funktionen, die an vielen Stellen
benötigt werden, die Daten usw. bereitgestellt werden.  Ferner werden,
wo möglich, Berechnungen mit dieser Initialisierung parallelisiert.
</p>

<div class="org-src-container">

<pre class="src src-R" id="statistic"><span style="color: #a2cd5a;">require</span>(MASS)
<span style="color: #a2cd5a;">library</span>(foreach)
<span style="color: #a2cd5a;">library</span>(doMC)
<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">number of cores to use</span>
registerDoMC(4)

calculationtime = proc.time()
komus = read.table(<span style="color: #deb887;">"_data/komus/data-komus-bin.csv"</span>,header=<span style="color: #98f5ff;">TRUE</span>) <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Perhaps breaks, it was in another format.</span>
test = data.frame(read.table(<span style="color: #deb887;">"_data/komus/data-komus-revised.csv"</span>,header=<span style="color: #98f5ff;">TRUE</span>, sep=<span style="color: #deb887;">','</span>))
pcitems = array(which(sapply(test, max) &gt; 1))
pcitems.temp = pcitems
test[pcitems] = lapply(test[pcitems],factor)

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">functions</span>
FUN.infoMC = <span style="color: #00bfff;">function</span>(x)
{
    <span style="color: #00bfff;">return</span>(rowSums(-x*log(x+0.0000000001, 2)))
}

FUN.infoMC.IND = <span style="color: #00bfff;">function</span>(x)
{
    <span style="color: #00bfff;">return</span>(sum(-x*log(x+0.0000000001, 2)))
}

FUN.info = <span style="color: #00bfff;">function</span>(x)
{
    <span style="color: #00bfff;">return</span>(-x*log(x+0.0000000001, 2)-(1-x)*log(1-x+0.0000000001, 2))
}

FUN.Odds = <span style="color: #00bfff;">function</span>(x)
{
    <span style="color: #00bfff;">if</span> (length(pcitems.temp) &gt; 0)
    {
        x[-pcitems.temp] = lapply(x[-pcitems.temp], predict, type=<span style="color: #deb887;">"response"</span>)
        x[pcitems.temp] = lapply(x[pcitems.temp], predict, type=<span style="color: #deb887;">"probs"</span>)
    } <span style="color: #00bfff;">else</span>
    {
        x = lapply(x, predict, type=<span style="color: #deb887;">"response"</span>)
    }
    <span style="color: #00bfff;">return</span>(x)
}

FUN.Odds.IND = <span style="color: #00bfff;">function</span>(x,y)
{
    x[-pcitems.temp] = lapply(x[-pcitems.temp], predict, data = test[y,], type=<span style="color: #deb887;">"response"</span>)
    x[pcitems.temp] = lapply(x[pcitems.temp], predict, data = test[y,], type=<span style="color: #deb887;">"probs"</span>)
    <span style="color: #00bfff;">return</span>(x)
}

FUN.info.temp = <span style="color: #00bfff;">function</span>(x)
{
    <span style="color: #00bfff;">if</span> (length(pcitems.temp) &gt; 0)
    {
        x[pcitems.temp] = lapply(x[pcitems.temp], FUN.infoMC)
        x[-pcitems.temp] =lapply(x[-pcitems.temp], FUN.info)
    } <span style="color: #00bfff;">else</span>   
    {
        x =lapply(x, FUN.info)
    }
    x = simplify2array(x)
    <span style="color: #00bfff;">return</span>(x)
}

FUN.info.temp.IND = <span style="color: #00bfff;">function</span>(x)
{
    <span style="color: #00bfff;">if</span> (length(pcitems.temp) &gt; 0)
    {
        x[pcitems.temp] = lapply(x[pcitems.temp], FUN.infoMC.IND)
        x[-pcitems.temp] =lapply(x[-pcitems.temp], FUN.info)
    } <span style="color: #00bfff;">else</span>
    {
        x =lapply(x, FUN.info)
    }
    x = simplify2array(x)
    <span style="color: #00bfff;">return</span>(x)
}

FUN.EntroMC = <span style="color: #00bfff;">function</span>(funpcitems.temp, fundata, funmod)
{
    <span style="color: #00bfff;">if</span> (length(funpcitems.temp) == 1)
    {
        odds = lapply(funmod, predict, fundata, type=<span style="color: #deb887;">"response"</span>)
        info.temp = odds
        info.temp = lapply(odds, FUN.info)
        info.temp = simplify2array(info.temp)
    } <span style="color: #00bfff;">else</span>
    {
        funpcitems.temp = funpcitems.temp[2:length(funpcitems.temp)]
        odds = funmod
        odds[-funpcitems.temp] = lapply(funmod[-funpcitems.temp], predict, fundata, type=<span style="color: #deb887;">"response"</span>)
        odds[funpcitems.temp] = lapply(funmod[funpcitems.temp], predict, fundata, type=<span style="color: #deb887;">"probs"</span>)
        info.temp = odds
        info.temp[funpcitems.temp] = lapply(odds[funpcitems.temp], FUN.infoMC.IND)
        info.temp[-funpcitems.temp] = lapply(odds[-funpcitems.temp], FUN.info)
        info.temp = simplify2array(info.temp)
    }
    <span style="color: #00bfff;">return</span>(info.temp)
}

odds = <span style="color: #98f5ff;">NULL</span>
fit = <span style="color: #98f5ff;">NULL</span>
modell = <span style="color: #98f5ff;">NULL</span>
summe = data.frame()

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">number of items and persons to consider in this calculation</span>
items = length(test)
persons = length(test[,1])

info = <span style="color: #98f5ff;">NULL</span>
info.rest = data.frame(matrix(ncol = 1, nrow = items+1))
info.rest.SD = data.frame(matrix(ncol = 1, nrow = items+1))
names(info.rest) = <span style="color: #deb887;">'kill'</span>
names(info.rest.SD) = <span style="color: #deb887;">'kill'</span>

entropie = data.frame(matrix(ncol = 1, nrow = items+1))
entropie.SD = data.frame(matrix(ncol = 1, nrow = items+1))
names(entropie) = <span style="color: #deb887;">'kill'</span>
names(entropie.SD) = <span style="color: #deb887;">'kill'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-3-1-2" class="outline-5">
<h5 id="sec-2-3-1-2"><span class="section-number-5">2.3.1.2</span> Nichtadaptiv</h5>
<div class="outline-text-5" id="text-2-3-1-2">
</div><ol class="org-ol"><li>Unbedingte und bedingte info in normaler Reihenfolge<br  /><div class="outline-text-6" id="text-2-3-1-2-1">
<p>
Dieser verhältnismäßig simple Code berechnet die info über die
klassische Itemschwierigkeit und die info über die durch
binär-logistische Regressionen vorhergesagte Itemschwierigkeit in der
ursprünglichen Reihenfolge.  Zudem wird bei zweiter Berechnung noch
angegeben, wie viel entropie.rest nach jeder Antwort noch zu erwarten
ist.
</p>
<div class="org-src-container">

<pre class="src src-R" id="statistic1">modell = <span style="color: #98f5ff;">NULL</span>
pcitems.temp = pcitems.temp[pcitems.temp &lt;= items]

<span style="color: #00bfff;">if</span> (1 %<span style="color: #00bfff;">in</span>% pcitems.temp)
{
    modell[[1]] = polr(reformulate(<span style="color: #deb887;">'1'</span>, names(test[1])), data = test)
} <span style="color: #00bfff;">else</span>
{
    modell[[1]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(test[1])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
}

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:items)
{
    <span style="color: #00bfff;">if</span> (i %<span style="color: #00bfff;">in</span>% pcitems.temp)
    {
        modell[[i]] = polr(reformulate(names(test[1:i-1]), names(test[i])), data = test)
    } <span style="color: #00bfff;">else</span>
    {
        modell[[i]] = glm(reformulate(names(test[1:i-1]), names(test[i])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
    }
}

fit = modell
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
odds = FUN.Odds(fit)
info.temp = FUN.info.temp(odds)

<span style="color: #7f7f7f;">### </span><span style="color: #7f7f7f;">Without relations ###</span>
fit = lapply(fit, update, ~ 1)
odds2 = FUN.Odds(fit)
info.temp2 = FUN.info.temp(odds2)
pcitems.temp = pcitems
query = <span style="color: #98f5ff;">NULL</span>
rest.temp = <span style="color: #98f5ff;">NULL</span>

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:items)
{
    info.temp3 = <span style="color: #98f5ff;">NULL</span>
    fit3 = <span style="color: #98f5ff;">NULL</span>

    <span style="color: #00bfff;">if</span> (i == length(test))
    {
        rest.temp[[i]] = rest.temp[[1]]*0
    } <span style="color: #00bfff;">else</span>
    {
        query = 1:i
        pcitems.temp = which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems]))

        <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(test[-query]))
        {
            <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
            {
                fit3[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
            } <span style="color: #00bfff;">else</span>
            {
                fit3[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
            }
        }

        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
        odds3 = FUN.Odds(fit3)
        info.temp3 = FUN.info.temp(odds3)
        rest.temp[[i]] = rowSums(info.temp3)
    }
}

rest.temp = simplify2array(rest.temp)
info.rest$bedunsort = c(0,colMeans(rest.temp))
info.rest.SD$bedunsort = c(0,apply(rest.temp, 2, sd))
entropie.SD.temp = sd(info.temp[,1])

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:length(info.temp[1,]))
{
    entropie.SD.temp[i] = sd(rowSums(info.temp[,1:i]))
}

entropie.SD$bedunsort = c(0,entropie.SD.temp)
entropie$bedunsort = c(0,colMeans(info.temp))
entropie$unbedunsort = c(0,colMeans(info.temp2))
entropie$unbedsort = c(0,sort(colMeans(info.temp2), decreasing =<span style="color: #98f5ff;">TRUE</span>))
info.temp2 = data.frame(info.temp2)
names(info.temp2) = names(test[1:length(info.temp2)])
entropie
info.rest
</pre>
</div>
</div>
</li>

<li>Bedingte, sortierte info<br  /><div class="outline-text-6" id="text-2-3-1-2-2">
<p>
Hier werden die Items schlicht nach dem durchschnittlichen infogehalt
sortiert, bevor die bedingte info mit Regressionen berechnet wird.
Dies verbessert die resultierende Kurve schon um einiges, der
infogewinn ist so tendenziell am Anfang weit höher als am Ende, trotz
dass gleich viel info innerhalb des kompletten Durchlaufes ermittelt
wurde.
</p>
<div class="org-src-container">

<pre class="src src-R" id="statistic2">modell = <span style="color: #98f5ff;">NULL</span>
odds = <span style="color: #98f5ff;">NULL</span>
fit = <span style="color: #98f5ff;">NULL</span>

<span style="color: #7f7f7f;">############## </span><span style="color: #7f7f7f;">sortierte Reihenfolge</span>
<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:items)
{
    <span style="color: #00bfff;">if</span> (i %<span style="color: #00bfff;">in</span>% pcitems.temp)
    {
        modell[[i]] = polr(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test)
    } <span style="color: #00bfff;">else</span>
    {
        modell[[i]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
    }
}

odds = FUN.Odds(modell)
info.temp = FUN.info.temp(odds)
info.temp = data.frame(info.temp)
names(info.temp) = names(test[1:length(info.temp)])
komus2 = test[c(names(sort(colMeans(info.temp), decreasing=<span style="color: #98f5ff;">TRUE</span>)))]
<span style="color: #7f7f7f;">#########</span>

names(sort(colMeans(info.temp), decreasing=<span style="color: #98f5ff;">TRUE</span>))
pcitems.temp.alt = pcitems.temp
pcitems.temp.alt
pcitems.temp = which(names(komus2) %<span style="color: #00bfff;">in</span>% names(test[pcitems.temp.alt]))
modell = <span style="color: #98f5ff;">NULL</span>
fit = <span style="color: #98f5ff;">NULL</span>
odds = <span style="color: #98f5ff;">NULL</span>

<span style="color: #00bfff;">if</span> (1 %<span style="color: #00bfff;">in</span>% pcitems.temp)
{
    modell[[1]] = polr(reformulate(<span style="color: #deb887;">'1'</span>, names(komus2[1])), data = komus2)
} <span style="color: #00bfff;">else</span>
{
    modell[[1]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(komus2[1])), data = komus2, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
}

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:items)
{
    <span style="color: #00bfff;">if</span> (i %<span style="color: #00bfff;">in</span>% pcitems.temp)
    {
        modell[[i]] = polr(reformulate(names(komus2[1:i-1]), names(komus2[i])), data = komus2)
    } <span style="color: #00bfff;">else</span> {
        modell[[i]] = glm(reformulate(names(komus2[1:i-1]), names(komus2[i])), data = komus2, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
    }
}

fit = modell
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
odds = FUN.Odds(fit)
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">odds[-pcitems.temp] = lapply(fit[-pcitems.temp], predict, type="response")</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">odds[pcitems.temp] = lapply(fit[pcitems.temp], predict, type="probs")</span>

<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">info.temp = fit</span>
info.temp = FUN.info.temp(odds)
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">info.temp[pcitems.temp] = lapply(odds[pcitems.temp], FUN.infoMC)</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">info.temp[-pcitems.temp] =lapply(odds[-pcitems.temp], FUN.info)</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">info.temp = simplify2array(info.temp)</span>

entropie.SD.temp = sd(info.temp[,1])

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:length(info.temp[1,]))
{
    entropie.SD.temp[i] = sd(rowSums(info.temp[,1:i]))
}

entropie.SD$sortbed = c(0,entropie.SD.temp)
entropie$sortbed = c(0,colMeans(info.temp))
pcitems.temp = pcitems.temp.alt
</pre>
</div>
</div>
</li>

<li>Durchschnittlich bedingtsortierte info<br  /><div class="outline-text-6" id="text-2-3-1-2-3">
<p>
Dieses Verfahren ist bereits weit rechenintensiver, es wird
nacheinander das Item ausgewählt, welches durchschnittlich die info am
meisten senkt.  Es wird also nach der Erfassung eines Items dieses
miteinbezogen für kommende Regressionen.  Insgesamt ist dies aber noch
nicht individualisiert und dementsprechen nicht adaptiv.
</p>
<div class="org-src-container">

<pre class="src src-R" id="statistic3">query = <span style="color: #98f5ff;">NULL</span>
modell = <span style="color: #98f5ff;">NULL</span>
rest.temp = <span style="color: #98f5ff;">NULL</span>
pcitems = pcitems.temp
<span style="color: #7f7f7f;">############## </span><span style="color: #7f7f7f;">sortierte Reihenfolge</span>
<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:length(test))
{
    <span style="color: #00bfff;">if</span> (i %<span style="color: #00bfff;">in</span>% pcitems.temp)
    {
        fit[[i]] = polr(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test)
    } <span style="color: #00bfff;">else</span>
    {
        fit[[i]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
    }
}
odds = FUN.Odds(fit)
info.temp = FUN.info.temp(odds)
query = which(names(test[which(colMeans(info.temp) == max(colMeans(info.temp)))[1]]) == names(test))[1]
query
modell[[1]] = fit[[query]]

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:items) {
    info.temp = <span style="color: #98f5ff;">NULL</span>
    fit = <span style="color: #98f5ff;">NULL</span>
    pcitems.temp = which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems]))

    <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(test[-query]))
    {
        <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
        {
            fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
        } <span style="color: #00bfff;">else</span>
        {
            fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
        }
    }
    <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
    <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
    odds = FUN.Odds(fit)
    info.temp = FUN.info.temp(odds)
    rest.temp[[i-1]] = rowSums(info.temp)
    query = c(query, which(names(test[-query][which(colMeans(info.temp) == max(colMeans(info.temp)))[1]]) == names(test))[1])
    modell[[i]] = fit[[which(colMeans(info.temp) == max(colMeans(info.temp)))[1]]]
}

<span style="color: #00bfff;">if</span> (length(test) == items)
{
    rest.temp[[items]] = rest.temp[[1]]*0
} <span style="color: #00bfff;">else</span>
{
    fit = <span style="color: #98f5ff;">NULL</span>
    pcitems.temp = which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems]))

    <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(test[-query]))
    {
        <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
        {
            fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
        } <span style="color: #00bfff;">else</span>
        {
            fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
        }
    }
    <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
    <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
    odds = FUN.Odds(fit)
    info.temp = FUN.info.temp(odds)
    rest.temp[[items]] = rowSums(info.temp)
}

pcitems.temp = which(query %<span style="color: #00bfff;">in</span>% pcitems)
rest.temp = simplify2array(rest.temp)
odds = FUN.Odds(modell)
info.temp = FUN.info.temp(odds)
entropie.SD.temp = sd(info.temp[,1])

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:length(info.temp[1,]))
{
    entropie.SD.temp[i] = sd(rowSums(info.temp[,1:i]))
}

entropie.SD$durchschbedsort = c(0,entropie.SD.temp)
entropie$durchschbedsort = c(0,colMeans(info.temp))
info.rest$durchschbedsort = c(0,colMeans(rest.temp))
info.rest.SD$durchschbedsort = c(0,apply(rest.temp, 2, sd))
</pre>
</div>
</div>
</li></ol>
</div>

<div id="outline-container-sec-2-3-1-3" class="outline-5">
<h5 id="sec-2-3-1-3"><span class="section-number-5">2.3.1.3</span> Adaptiv</h5>
<div class="outline-text-5" id="text-2-3-1-3">
</div><ol class="org-ol"><li>Individuellbedingtsortierte info<br  /><div class="outline-text-6" id="text-2-3-1-3-1">
<p>
Hier wird das zuletzt genannte Verfahren individualisiert, was den
Rechenaufwand in diesem Fall 319 mal höher macht.  Das Ergebniss ist
jedoch bereits ein echt adaptiver Test.  Somit ist die infokurve nun
auch viel stärker gekrümmt (hat also eine größere zweite Ableitung).
Somit kann unter kleinem Informationsverlust der Test stark verkürzt
werden.
</p>

<p>
Ideal wäre ein Itempool, der nicht komplett erschöpft wird in einer
Testung. Somit könnte man berechnen, wie lang ein nichtadaptiver im
Vergleich zu einem gleichpräzisen adaptiven Test ist.
</p>
<div class="org-src-container">

<pre class="src src-R" id="statistic4"><span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">initializing</span>
infoall = <span style="color: #98f5ff;">NULL</span>
odds = <span style="color: #98f5ff;">NULL</span>
rest.temp = <span style="color: #98f5ff;">NULL</span>
query = <span style="color: #98f5ff;">NULL</span>
modell = <span style="color: #98f5ff;">NULL</span>
rest.temp = <span style="color: #98f5ff;">NULL</span>
pcitems.temp = pcitems
fit = <span style="color: #98f5ff;">NULL</span>

<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">first item</span>
<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:length(test))
{
    <span style="color: #00bfff;">if</span> (i %<span style="color: #00bfff;">in</span>% pcitems.temp)
    {
        fit[[i]] = polr(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test)
    } <span style="color: #00bfff;">else</span>
    {
        fit[[i]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
    }
}

odds = fit
odds[-pcitems.temp] = lapply(fit[-pcitems.temp], predict, test[1,], type=<span style="color: #deb887;">"response"</span>)
odds[pcitems.temp] = lapply(fit[pcitems.temp], predict, test[1,], type=<span style="color: #deb887;">"probs"</span>)
info.temp = FUN.info.temp.IND(odds)
query = which(names(test[which(info.temp == max(info.temp))[1]]) == names(test))[1]
modell[[1]] = fit[[query]]
queryinit = query
fit = <span style="color: #98f5ff;">NULL</span>

<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">multicorecalculation for every person</span>
infoall = simplify2array(foreach(k=1:persons) %dopar%
{
    query = queryinit
    entropie.rest = <span style="color: #98f5ff;">NULL</span>

    <span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:items)
    {
        odds = <span style="color: #98f5ff;">NULL</span>
        info.temp = <span style="color: #98f5ff;">NULL</span>
        fit = <span style="color: #98f5ff;">NULL</span>
        pcitems.temp = c(0,which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems])))

        <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(test[-query]))
        {
            <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
            {
                fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
            } <span style="color: #00bfff;">else</span>
            {
                fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
            }
        }

        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
        odds = fit
        info.temp = FUN.EntroMC(pcitems.temp,test[k,], fit)
        rest.temp[i-1] = sum(info.temp) <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">rest of entropie before this item</span>
        query = c(query, which(names(test[-query][which(info.temp == max(info.temp))[1]]) == names(test))[1])
        modell[[i]] = fit[[which(info.temp == max(info.temp))[1]]]
    }

    <span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">calculation of last rest entropie</span>
    <span style="color: #00bfff;">if</span> (length(test) == items)
    {
        rest.temp[items] = 0
    } <span style="color: #00bfff;">else</span>
    {
        fit = <span style="color: #98f5ff;">NULL</span>
        pcitems.temp = 0
        pcitems.temp = c(0,which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems])))

        <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(test[-query]))
        {
            <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
            {
                fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
            } <span style="color: #00bfff;">else</span>
            {
                fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
            }
        }
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
        info.temp = FUN.EntroMC(pcitems.temp,test[k,], fit)
        rest.temp[items] = sum(info.temp)
    }

    <span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">calculation of the choosen modell</span>
    pcitems.temp = c(0,which(query %<span style="color: #00bfff;">in</span>% pcitems))
    info.temp = FUN.EntroMC(pcitems.temp,test[k,], modell)
    <span style="color: #00bfff;">return</span>(c(info.temp, rest.temp))
})

rest.temp = (infoall[(items+1):(items*2),])
infoall = infoall[1:items,]
entropie.SD.temp = sd(infoall[1,])

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:length(infoall[,1]))
{
    entropie.SD.temp[i] = sd(colSums(infoall[1:i,]))
}

entropie.SD$indivbedsort = c(0,entropie.SD.temp)
entropie$indivbedsort = c(0,rowMeans(infoall))
info.rest$indivbedsort = c(0,rowMeans(rest.temp))
info.rest.SD$indivbedsort = c(0,apply(rest.temp, 1, sd))
pcitems.temp = pcitems
</pre>
</div>
</div>
</li>

<li>Individuellbedingtsortierte info mit Trennschärfe<br  /><div class="outline-text-6" id="text-2-3-1-3-2">
<p>
Ein nicht gut gelungener Versuch, nicht nur die info als
Auswahlkriterium zu nehmen. Dies ist deswegen sinnvoll, da Items
vorstellbar sind mit hoher info, die aber mit dem Test wenig zu tun
haben (z.B. eine Frage nach der Schuhgröße hat vermutlich eine sehr
hohe info, hat aber vermutlich wenig mit musikalischer Kompetenz zu
tun).  Somit macht das bisherige Verfahren die Annahme, dass der
Itempool sehr gut konstruiert ist.  Dementsprechend kann man das
bisherige Verfahren sicher nicht als robust bezeichnen.
</p>
<div class="org-src-container">

<pre class="src src-R" id="statistic5">infoall = <span style="color: #98f5ff;">NULL</span>
odds = <span style="color: #98f5ff;">NULL</span>
beta = <span style="color: #98f5ff;">NULL</span>
rest.temp = <span style="color: #98f5ff;">NULL</span>
info.temp = <span style="color: #98f5ff;">NULL</span>
fit = <span style="color: #98f5ff;">NULL</span>

<span style="color: #00bfff;">if</span> (!exists(<span style="color: #deb887;">"information"</span>))
{
    information = simplify2array(foreach(m=1:length(komus)) %dopar%
    {
        <span style="color: #00bfff;">for</span> (n <span style="color: #00bfff;">in</span> 1:(length(komus)-1))
        {
            beta[[n]] = glm(reformulate(names(komus[m]), names(komus[-m][n])), data = komus, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
        }
        odds = simplify2array(lapply(beta, predict, type=<span style="color: #deb887;">"response"</span>))
        chancetemp = unlist(lapply(komus[m],mean))
        info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2))
        information = sum(colMeans(info.temp)) + (-chancetemp*log(chancetemp,2)-(1-chancetemp)*log(1-chancetemp,2))
        <span style="color: #00bfff;">return</span>(information)
    })
    information = -(information - sum(-colMeans(komus)*log(colMeans(komus),2)-(1-colMeans(komus))*log(1-colMeans(komus),2)))
}

<span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(komus))
{
    fit[[j]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(komus[j])), data = komus, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
}

<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
odds = simplify2array(lapply(fit, predict, komus[1,], type=<span style="color: #deb887;">"response"</span>))
info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2)) + (information)
queryinit = which(names(komus[which((info.temp) == max((info.temp)))[1]]) == names(komus))[1]
modell[[1]] = fit[[which((info.temp) == max((info.temp)))[1]]]

infoall = simplify2array(foreach(k=1:persons) %dopar%
{
    query = queryinit

    <span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:items)
    {
        info.temp = <span style="color: #98f5ff;">NULL</span>
        fit = <span style="color: #98f5ff;">NULL</span>

        <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(komus[-query]))
        {
            fit[[j]] = glm(reformulate(names(komus[query]), names(komus[-query][j])), data = komus, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
        }

        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
        <span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">TODO stimmt das so?</span>
        odds = simplify2array(lapply(fit, predict, komus[k,], type=<span style="color: #deb887;">"response"</span>))
        rest.temp[i-1] = sum(-odds*log(odds,2)-(1-odds)*log(1-odds,2)) 
        info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2)) + (information[-query]*(1 - (length(query)+1)/items))
        query = c(query, which(names(komus[-query][which((info.temp) == max((info.temp)))[1]]) == names(komus))[1])
        modell[[i]] = fit[[which((info.temp) == max((info.temp)))[1]]]
    }

    <span style="color: #00bfff;">if</span> (length(komus) == items)
    {
        rest.temp[items] = 0
    } <span style="color: #00bfff;">else</span>
    {
        fit = <span style="color: #98f5ff;">NULL</span>

        <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(komus[-query]))
        {
            fit[[j]] = glm(reformulate(names(komus[query]), names(komus[-query][j])), data = komus, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
        }
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
        odds = simplify2array(lapply(fit, predict, komus[k,], type=<span style="color: #deb887;">"response"</span>))
        rest.temp[length(query)] = sum(-odds*log(odds,2)-(1-odds)*log(1-odds,2))
    }

    odds = simplify2array(lapply(modell, predict, komus[k,], type=<span style="color: #deb887;">"response"</span>))
    info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2))
    <span style="color: #00bfff;">return</span>(c(info.temp, rest.temp))
})

rest.temp = (infoall[(items+1):(items*2),])
infoall = infoall[1:items,]
entropie.SD.temp = sd(infoall[1,])

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:length(infoall[,1]))
{
    entropie.SD.temp[i] = sd(colSums(infoall[1:i,]))
}

entropie.SD$indivbedsorttrenn = c(0,entropie.SD.temp )
entropie$indivbedsorttrenn = c(0,rowMeans(infoall))
info.rest$indivbedsorttrenn = c(0,rowMeans(rest.temp))
info.rest.SD$indivbedsorttrenn = c(0,apply(rest.temp,1 ,sd))
</pre>
</div>
</div>
</li>

<li>Individuellbedingtsortierte info mit Prädiktion<br  /><div class="outline-text-6" id="text-2-3-1-3-3">
<p>
Hier wird nun die info rekursiv berechnet.  Es wird nicht nur
geschaut, welches Item die meiste info besitzt, sondern es werden für
jedes Item alle Antwortmöglichkeiten simuliert und mit dieser
Simulation die verbleibende info im gesamten Test errechnet, diese mit
der Chance der simulierten Antwort gewichtet und aufaddiert mit den
gewichteten anderen Antwortmöglichkeiten.
</p>

<p>
Dieses Modell umgeht also das Problem der vorherigen beiden.  Es ist
sehr robust, weil immer auch berechnet wird, wie sehr sich das
auserwählte Item mit all seinen Antwortmöglichkeiten auf die gesamte
entropie.rest auswirkt.  Dies ist eine mächtigere Form der
Trennschärfe, weil sie nicht starr, sondern antwortmusterspezifisch
ist.
</p>

<p>
Dieses Modell bringt die rechnerischen Anforderungen auf ein neues
Niveau, sie werden nochmals ungefähr 30 mal höher.  Als Konsequenz
daraus habe ich hier eine Datenbank mit implementiert, die einerseits
bereits berechnetes speichert um mir wiederholte Arbeit zu ersparen
und andererseits stets schaut, ob Frage-Antwort-Kombinationen bereits
bei anderen Schülern vorgekommen ist, um mit Hilfe dieses Wissens hin
und wieder einzelne Rechnungen zu ersparen.
</p>

<p>
Zunächst könnte man denken, dass es bei rund 50 binären Items \(2^{50}\)
Möglichkeiten der Antwortmuster gibt, was die Datenbank als sinnlos
erscheinen lässt.  Jedoch muss bedacht werden, dass die Antwort
Reihenfolge in der aktuellen Regression keine Rolle
spielt. Beantwortet man Item a, b, und c richtig und bekommt daraufhin
Item c, so würde man dies genauso bekommen, wenn man b, c und dann
erst a richtig beantwortet, was die Sinnhaftigkeit der Datenbank
deutlich steigert.  Zudem werden manche Antwortmuster und manche Items
gehäuft vorkommen, weil sie entweder besonders qualitativ, oder
besonders normal sind.  Im Moment fangen beispielsweise alle Schüler
mit dem gleichen, maximal informativen Item an, weil noch keine
Vorinformation über die Schüler vorhanden ist.
</p>

<div class="org-src-container">

<pre class="src src-R" id="statistic6"><span style="color: #7f7f7f;">##### </span><span style="color: #7f7f7f;">Calculate the benefit of an adaptive test with non-adaptive data.</span>
<span style="color: #7f7f7f;">#</span>
<span style="color: #7f7f7f;">#  </span><span style="color: #7f7f7f;">Use data.dat to predict answers of items.</span>
<span style="color: #7f7f7f;">#  </span><span style="color: #7f7f7f;">Return a database (database.dat) of proposed items for each individual.</span>
<span style="color: #7f7f7f;">#  </span><span style="color: #7f7f7f;">Draw a curve of the linear and adaptive test.</span>
<span style="color: #7f7f7f;">#  </span><span style="color: #7f7f7f;">Afterwards, search.sh should be run to update the database.</span>
<span style="color: #7f7f7f;"># </span>
<span style="color: #7f7f7f;">#####</span>

pcitems.temp = pcitems
fit = <span style="color: #98f5ff;">NULL</span>
modell = <span style="color: #98f5ff;">NULL</span>
infoall = matrix(nrow=persons, ncol=items)

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Return a fitted modell, which considers polytomus and dichotomus items.</span>
FUN.modelliteration = <span style="color: #00bfff;">function</span>(fun.fit.formula, fun.pcitems, fun.iterator)
{
    <span style="color: #00bfff;">if</span> (fun.iterator %<span style="color: #00bfff;">in</span>% fun.pcitems)
    {
        fun.tempfit = polr(fun.fit.formula, data = test)
    } <span style="color: #00bfff;">else</span>
    {
        fun.tempfit = glm(fun.fit.formula, data = test, family = <span style="color: #deb887;">"binomial"</span>)
    }
    <span style="color: #00bfff;">return</span>(fun.tempfit)
}

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Binary search in database.dat for each column to match already calculated predictions.</span>
FUN.binarysearch = <span style="color: #00bfff;">function</span>(query)
{
    <span style="color: #00bfff;">if</span> (!exists(<span style="color: #deb887;">"database"</span>))
    {
        <span style="color: #00bfff;">return</span>(0)
    }

    lower = 1
    upper = length(database[, 1])
    current = upper

    <span style="color: #00bfff;">while</span> (upper &gt;= lower)
    {
        current = round((lower+upper) / 2)

        <span style="color: #00bfff;">for</span> (u <span style="color: #00bfff;">in</span> 1:length(query))
        {
            <span style="color: #00bfff;">if</span> (answers.sorted[u] &gt; database[current, u])
            {
                lower = current + 1
                <span style="color: #00bfff;">break</span>
            } <span style="color: #00bfff;">else</span> <span style="color: #00bfff;">if</span> (answers.sorted[u] &lt; database[current, u])
            {
                upper = current - 1
                <span style="color: #00bfff;">break</span>
            } <span style="color: #00bfff;">else</span> <span style="color: #00bfff;">if</span> (u == length(query))
            {
                <span style="color: #00bfff;">if</span> (!database[current, (u+1)])
                {
                    <span style="color: #00bfff;">return</span>(current)
                } <span style="color: #00bfff;">else</span>
                {
                    upper = current - 1
                    <span style="color: #00bfff;">break</span>
                }
            }
        }
    }

    <span style="color: #00bfff;">return</span>(0)
}

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Linear search in new data, if binary search fails.</span>
<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">This is a lot slower and should be only used, if unsorted database is small.</span>
FUN.unsortedsearch = <span style="color: #00bfff;">function</span>(query)
{
    <span style="color: #00bfff;">if</span> (exists(<span style="color: #deb887;">"newdata"</span>))
    {
        <span style="color: #00bfff;">for</span> (m <span style="color: #00bfff;">in</span> 1:length(newdata[, 1]))
        {
            <span style="color: #00bfff;">for</span> (u <span style="color: #00bfff;">in</span> 1:length(query))
            {
                <span style="color: #00bfff;">if</span> (answers.sorted[u] != newdata[m, u])
                {
                    <span style="color: #00bfff;">break</span>
                }

                <span style="color: #00bfff;">if</span> (!newdata[m,(length(query)+3)] &amp;&amp; u == length(query))
                {
                    <span style="color: #00bfff;">return</span>(m)
                }
            }
        }
    }
    <span style="color: #00bfff;">return</span>(0)
}

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Return the modell for the first item.</span>
<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:length(test)) 
{
    fit.formula = reformulate(<span style="color: #deb887;">'1'</span>, names(test[i]))
    fit[[i]] = FUN.modelliteration(fit.formula, pcitems, i)
}

odds = fit
odds[-pcitems.temp] = lapply(fit[-pcitems], predict, test[1,], type=<span style="color: #deb887;">"response"</span>)
odds[pcitems.temp] = lapply(fit[pcitems], predict, test[1,], type=<span style="color: #deb887;">"probs"</span>)
info.temp = FUN.info.temp.IND(odds)
query = which(names(test[which(info.temp == max(info.temp))[1]]) == names(test))[1]
modell[[1]] = fit[[query]]
queryinit = query

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Calculate item predictions for each person for all remaining items.</span>
<span style="color: #00bfff;">for</span> (k <span style="color: #00bfff;">in</span> 1:persons)
{
    query = queryinit
    entropie.rest = <span style="color: #98f5ff;">NULL</span>
    rest.temp = <span style="color: #98f5ff;">NULL</span>
    rest.temp2 = <span style="color: #98f5ff;">NULL</span>

    calcu = 0
    calcutime = proc.time()
    answers = vector(length = (length(test)+2))
    answers[1] = query[1] + as.numeric(as.character(test[k, query[1]]))/100

    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Check existing databases.</span>
    <span style="color: #00bfff;">if</span> (file.exists(<span style="color: #deb887;">'_data/komus/sorted-database.dat'</span>))
    {
        database = read.table(<span style="color: #deb887;">'_data/komus/sorted-database.dat'</span>)
    }

    <span style="color: #00bfff;">if</span> (file.exists(<span style="color: #deb887;">'_data/komus/newdata.dat'</span>))
    {
        newdata = read.table(<span style="color: #deb887;">'_data/komus/newdata.dat'</span>)
    }

    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Calculate next item.</span>
    <span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 2:items)
    {
        odds = <span style="color: #98f5ff;">NULL</span>
        info.temp = <span style="color: #98f5ff;">NULL</span>
        info.temp2 = <span style="color: #98f5ff;">NULL</span>
        fit = <span style="color: #98f5ff;">NULL</span>
        fit2 = <span style="color: #98f5ff;">NULL</span>
        pcitems.temp = c(0, which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems])))
        answers.sorted = sort(answers[1:length(query)])

        <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Search in database if pattern of answers already exists.</span>
        found = FUN.binarysearch(query)

        <span style="color: #00bfff;">if</span> (found)
        {
            lq = length(query)
            rest.temp[i-1] = database[found, (lq+2)]
            found = database[found, (lq+3)]
            query = c(query, found)
        } <span style="color: #00bfff;">else</span>
        {
            found = FUN.unsortedsearch(query)
            <span style="color: #00bfff;">if</span> (found)
            {
                lq = length(query)
                rest.temp[i-1] = newdata[found, (lq+1)]
                found = newdata[found, (lq+2)]
                query = c(query, found)
            }
        }

        <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">If pattern isn't found, calculate the new one.</span>
        <span style="color: #00bfff;">if</span> (!found)
        {
            calcu = calcu+1
            isgood = <span style="color: #98f5ff;">NULL</span>

            <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Create a modell for each remaining item with all answers as predictors.</span>
            <span style="color: #00bfff;">for</span> (q <span style="color: #00bfff;">in</span> 1:length(test[-query]))
            {
                fit.formula = reformulate(names(test[query]), names(test[-query][q]))
                fit[[q]] = FUN.modelliteration(fit.formula, pcitems.temp, q)
            }

            <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Calculate the entropie of these items and choose good ones.</span>
            info.temp = FUN.EntroMC(pcitems.temp, test[k, ], fit)
            isgood = info.temp &gt;= (max(info.temp)[1] * 0.8)

            <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Multicore calculation.</span>
            Liste = foreach(j=1:length(test[-query])) %dopar%
            {
                <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Give the probability of each answer possibility.</span>
                <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
                {
                    chance = predict(fit[[j]], test[k,], type=<span style="color: #deb887;">"probs"</span>)
                } <span style="color: #00bfff;">else</span>
                {
                    chance = predict(fit[[j]], test[k,], type=<span style="color: #deb887;">"response"</span>)
                    chance[2] = 1-chance[1]
                }

                rest.temp2 = <span style="color: #98f5ff;">NULL</span>

                <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Predict the entropie with simulated answers of good items.</span>
                <span style="color: #00bfff;">if</span> (length(test[-query]) &gt; 1 &amp;&amp; isgood[j])
                {
                    pcitems.temp2 = c(0, which(names(test[-query][-j]) %<span style="color: #00bfff;">in</span>% names(test[pcitems])))

                    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Calculate the fitted modell of each item.</span>
                    <span style="color: #00bfff;">for</span> (n <span style="color: #00bfff;">in</span> 1:length(test[-query][-j]))
                    {
                        fit2.formula = reformulate(names(c(test[query], test[-query][j])), names(test[-query][-j][n]))
                        fit2[[n]] = FUN.modelliteration(fit2.formula, pcitems.temp2, n)
                    }

                    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Calculate the reduction of entropie multiplied by the chance of these answers.</span>
                    tempdata = test[k, ]

                    <span style="color: #00bfff;">if</span> (j %<span style="color: #00bfff;">in</span>% pcitems.temp)
                    {
                        <span style="color: #00bfff;">for</span> (s <span style="color: #00bfff;">in</span> 1:length(chance))
                        {
                            tempdata[-query][j] = factor(s-1)
                            info.temp = FUN.EntroMC(pcitems.temp2, tempdata, fit2)*chance[s]
                            rest.temp2[s] = sum(info.temp)
                        }
                        rest.temp2 = sum(rest.temp2)
                    } <span style="color: #00bfff;">else</span>
                    {
                        tempdata[-query][j] = 1
                        info.temp = FUN.EntroMC(pcitems.temp2, tempdata, fit2)*chance[1]
                        rest.temp2 = sum(info.temp)
                        tempdata[-query][j] = 0
                        info.temp = FUN.EntroMC(pcitems.temp2, tempdata, fit2)*chance[2]
                        rest.temp2[2] = sum(info.temp)
                        rest.temp2 = sum(rest.temp2)
                    }

                    fit2 = <span style="color: #98f5ff;">NULL</span>
                } <span style="color: #00bfff;">else</span>
                {
                    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">If the item is the last one, take it.</span>
                    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">If the item isn't good, don't take it.</span>
                    <span style="color: #00bfff;">if</span> (isgood[j])
                    {
                        rest.temp2 = 0

                    } <span style="color: #00bfff;">else</span> 
                    {
                        rest.temp2 = 55555555
                    }
                }

                <span style="color: #00bfff;">return</span>(rest.temp2)
            }

            <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Save the best modell, add the proposed item to the list of questions.</span>
            rest.temp2 = simplify2array(Liste)
            query = c(query, which(names(test[-query][which(rest.temp2 == min(rest.temp2))[1]]) == names(test))[1])
            modell[[i]] = fit[[which(rest.temp2 == min(rest.temp2))[1]]]
            <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
            <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
            odds = fit
            info.temp = FUN.EntroMC(pcitems.temp, test[k,], fit)
            rest.temp[i-1] = sum(info.temp)
        }

        answers[i] = query[i] + as.numeric(as.character(test[k, query[i]]))/100

        <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Save the new calculated pattern to the database.</span>
        <span style="color: #00bfff;">if</span> (!found) 
        {
            temp = answers
            temp[1:(i-1)] = sort(answers[1:(i-1)])
            temp[i] = rest.temp[i-1]
            temp[i+1] = query[i]
            write(temp, file=<span style="color: #deb887;">'_data/komus/newdata.dat'</span>, append=<span style="color: #98f5ff;">TRUE</span>, ncolumns=length(answers))
        }

        plot(rest.temp, type=<span style="color: #deb887;">'l'</span>, col=rgb(0, 0.7, 0.7))
    }

    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Calculate the last remaining entropie (which may be non-nil if not all items are answered).</span>
    <span style="color: #00bfff;">if</span> (length(test) == items)
    {
        rest.temp[items] = 0
    } <span style="color: #00bfff;">else</span>
    {
        fit = <span style="color: #98f5ff;">NULL</span>
        pcitems.temp = c(0, which(names(test[-query]) %<span style="color: #00bfff;">in</span>% names(test[pcitems])))

        <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(test[-query]))
        {
            fit.formula = reformulate(names(test[query]), names(test[-query][j]))
            fit[[j]] = FUN.modelliteration(fit.formula, pcitems.temp, j)
        }

        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
        <span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
        info.temp = FUN.EntroMC(pcitems.temp, test[k, ], fit)
        rest.temp[items] = sum(info.temp)
    }

    <span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Save the number of new patterns and seconds for this person.</span>
    pcitems.temp = c(0, which(query %<span style="color: #00bfff;">in</span>% pcitems))

    <span style="color: #00bfff;">if</span> (calcu)
    {
        temp = vector(length=(length(test)+2))
        temp[2] = k
        temp[3] = (proc.time() - calcutime)[3]
        temp[4] = calcu
        write(temp, file=<span style="color: #deb887;">'_data/komus/newdata.dat'</span>, append=<span style="color: #98f5ff;">TRUE</span>, ncolumns=length(answers))
    }

    infoall[k,] = rest.temp
}

<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Clean-up.</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">restt = (infoall[(items+1):(items*2)&#252;,])</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">infoall = infoall[1:items,]</span>
<span style="color: #7f7f7f;">## </span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">entropie.SD.temp = sd(infoall[1,])</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">for (i in 2:length(infoall[,1])) {</span>
<span style="color: #7f7f7f;">##     </span><span style="color: #7f7f7f;">entropie.SD.temp[i] = sd(colSums(infoall[1:i,]))</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">}</span>
<span style="color: #7f7f7f;">## </span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">entropie.SD$indivbedsortpred = c(0, entropie.SD.temp)</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">entropie$indivbedsortpred = c(0, rowMeans(infoall))</span>
info.rest$indivbedsortpred = c(0, colMeans(infoall))
info.rest.SD$indivbedsortpred = c(0, apply(infoall, 2, sd))

pcitems.temp = pcitems
<span style="color: #7f7f7f;"># </span><span style="color: #7f7f7f;">Give some output for debugging.</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">entropie</span>
<span style="color: #7f7f7f;">## </span><span style="color: #7f7f7f;">entropie.SD</span>
info.rest
info.rest.SD
</pre>
</div>
</div>
</li></ol>
</div>

<div id="outline-container-sec-2-3-1-4" class="outline-5">
<h5 id="sec-2-3-1-4"><span class="section-number-5">2.3.1.4</span> Schlussberechnungen</h5>
<div class="outline-text-5" id="text-2-3-1-4">
<p>
Hier werden lediglich ein paar Aufräumarbeiten in den Daten noch
erledigt, um diese dann gut zeichnen zu können.
</p>
<div class="org-src-container">

<pre class="src src-R" id="statisticend"><span style="color: #00bfff;">if</span> (names(entropie[1]) == <span style="color: #deb887;">'kill'</span>)
{
    entropie = entropie[-1]
}

<span style="color: #00bfff;">if</span> (names(entropie.SD[1]) == <span style="color: #deb887;">'kill'</span>)
{
    entropie.SD = entropie.SD[-1]
}

<span style="color: #00bfff;">if</span> (names(info.rest[1]) == <span style="color: #deb887;">'kill'</span>)
{
    info.rest = info.rest[-1]
    info.rest.SD = info.rest.SD[-1]
}

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:length(entropie[1,]))
{
    <span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:length(entropie[,1]))
    {
        summe[j,i] = sum(entropie[1:j,i])
    }
}

fit = <span style="color: #98f5ff;">NULL</span>
pcitems.temp = pcitems

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:length(test))
{
    <span style="color: #00bfff;">if</span> (i %<span style="color: #00bfff;">in</span>% pcitems.temp)
    {
        fit[[i]] = polr(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test)
    } <span style="color: #00bfff;">else</span>
    {
        fit[[i]] = glm(reformulate(<span style="color: #deb887;">'1'</span>, names(test[i])), data = test, family = <span style="color: #deb887;">"binomial"</span>(link=logit))
    }
}

odds = fit
odds[-pcitems.temp] = lapply(fit[-pcitems.temp], predict, test[1,], type=<span style="color: #deb887;">"response"</span>)
odds[pcitems.temp] = lapply(fit[pcitems.temp], predict, test[1,], type=<span style="color: #deb887;">"probs"</span>)
info.temp = FUN.info.temp.IND(odds)
info.rest[1,] = sum(info.temp)

names(summe) = names(entropie)

<span style="color: #00bfff;">if</span> (exists(<span style="color: #deb887;">"benchmark"</span>))
{
    benchmark = array(c(benchmark,(proc.time() - calculationtime)[3]))
} <span style="color: #00bfff;">else</span>
{
    benchmark = (proc.time() - calculationtime)[3]
}
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-3-1-5" class="outline-5">
<h5 id="sec-2-3-1-5"><span class="section-number-5">2.3.1.5</span> Formel für die Modellanpassung</h5>
<div class="outline-text-5" id="text-2-3-1-5">
<p>
Hier kann noch bestimmt werden, ob die binärlogistischen Regressionen
noch schlechte Items verwerfen, oder einfach mit allen rechnen.
Änderungen, die hier gemacht werden, werden automatisch im gesamten
Code angepasst, da dieser Teil mit noweb-syntax eingebunden ist.
</p>

<p>
Aus statistischer Sicht ist es natürlich viel besser, wenn schlechte
Items noch verworfen und noch Interaktionen hinzugefügt werden.  Was
hier aber dagegen spricht, ist die dadurch resultierende
Berechnungsdauer.  So sind selbst die einfacheren obigen Modell auch
nach Stunden nicht fertig.
</p>
<div class="org-src-container">

<pre class="src src-R" id="fit"><span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, trace = 0)</span>
<span style="color: #7f7f7f;">#</span><span style="color: #7f7f7f;">fit = lapply(fit, step, ~.^2, trace = 0)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-3-1-6" class="outline-5">
<h5 id="sec-2-3-1-6"><span class="section-number-5">2.3.1.6</span> Benchmark</h5>
<div class="outline-text-5" id="text-2-3-1-6">
<div class="org-src-container">

<pre class="src src-R">plot(benchmark, type=<span style="color: #deb887;">"l"</span>, col=rgb(0,0,0), ann=F)
title(xlab=<span style="color: #deb887;">"Durchlauf"</span>)
title(ylab=<span style="color: #deb887;">"Dauer"</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2-3-1-7" class="outline-5">
<h5 id="sec-2-3-1-7"><span class="section-number-5">2.3.1.7</span> infografik</h5>
<div class="outline-text-5" id="text-2-3-1-7">
<p>
Hier ist noch ein letztes kleines Bisschen an Code, welches die
derzeit kalkulierten Ergebnisse in eine Grafik packt.  Zudem werden
eine Legende generiert und die Berechnungsdauer angegeben.
</p>
<div class="org-src-container">

<pre class="src src-R" id="grafik">farbe = <span style="color: #98f5ff;">NULL</span>
farbeSD = <span style="color: #98f5ff;">NULL</span>
<span style="color: #00bfff;">for</span> (j <span style="color: #00bfff;">in</span> 1:(length(summe[1,])))
{
    r = runif(1,0.1,0.9)
    g = runif(1,0.1,0.9)
    b = runif(1,0.1,0.9)
    farbe[j] = rgb(r^1.2, g^1.2, b^1.2)
    farbeSD[j] = rgb(sqrt(r), sqrt(g), sqrt(b))
}

plot(0:(length(test)), type=<span style="color: #deb887;">"l"</span>, col=rgb(0,0,0), ann=F)

<span style="color: #00bfff;">for</span> (i <span style="color: #00bfff;">in</span> 1:(length(summe[1,])))
{
    lines(summe[,i], col=farbe[i])

    <span style="color: #00bfff;">if</span> (dim(entropie.SD[names(entropie.SD) == names(summe[i])])[2] != 0)
    {
        lines(summe[,i]+entropie.SD[names(summe[i])],lty = 4, col=farbeSD[i])
        lines(summe[,i]-entropie.SD[names(summe[i])],lty = 4, col=farbeSD[i])
    }

    <span style="color: #00bfff;">if</span> (dim(info.rest[names(info.rest) == names(summe[i])])[2] != 0)
    {
        lines(info.rest[names(summe[i])], col=farbe[i])
        lines(info.rest[names(summe[i])]+info.rest.SD[names(summe[i])],lty = 4, col=farbeSD[i])
        lines(info.rest[names(summe[i])]-info.rest.SD[names(summe[i])],lty = 4, col=farbeSD[i])
    }
}

title(xlab=<span style="color: #deb887;">"Anzahl der beantworteten Fragen"</span>)
title(ylab=<span style="color: #deb887;">"Entropie in bit"</span>)
legend(length(test)/4, length(test), c(names(summe), round(benchmark[length(benchmark)])), cex=0.9, col=c(farbe, rgb(1,1,1)), lty=1)
</pre>
</div>
</div>
</div>
</div>
</div>


