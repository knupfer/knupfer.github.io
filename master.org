* About
[2014-07-30 Wed]
** Einleitung
Lieber Leser,

hier findet sich nun also mein CV zur Einsicht.  Zu den in CVs
üblichen Inhalten gibt es hier noch Raum für Informelleres, meine
Hobbies sind die folgenden:

1. Kochen (vor allem exotische Küche)

2. Sprachen
     1. Englisch
     2. Spanisch
     3. Französisch
     4. Russisch
     5. Nächste anvisierte Sprachen:
          1. Arabisch
          2. Chinesisch
          3. Koreanisch

3. Reisen
4. Typographie
5. Technik
     1. Tablets
     2. Elektroautos
     3. Energieproduktion
          1. Wasserkraft
          2. Windkraft
          3. Kernenergie
               1. Fussionsreaktoren
               2. Fissionsreaktoren
6. Musik
     1. Orgel
          1. Literatur
               1. Durufle
               2. Couperin
               3. Reger
          2. Improvisation
               1. Impressionismus
               2. Romantik
     2. Gesang
          1. Bass
               1. Monteverdi
               2. Bach
               3. Schubert
          3. Altus
               1. Monteverdi
               2. Viadana
               3. Xenakis

** Curriculum Vitae
*** Ausbildung
|---+-----------+------------------------------------------------------|
|   |       <r> | <52>                                                 |
|   |  Aug 2013 | Staatsexamensarbeit: »Abgrenzung von Musikalität und musikalischer Kompetenz – Eine empirische Untersuchung von Over- und Underachievement bei Sechstklässlern«; Note 1,0 |
|   | seit 2010 | Physikstudium an der Universität Vaihingen           |
|   | seit 2008 | Doppelstudium Schulmusik und Kirchenmusik an der Hochschule für Musik und Darstellende Kunst Stuttgart |
|   | 2007–2008 | C-Kurs Rottenburg am Neckar                          |
| / | 1994–1998 | Grundschule Parkschule Essingen                      |
|---+-----------+------------------------------------------------------|

*** Preise und Stipendien
|---+-----------+------------------------------------------------------|
|   |       <r> | <52>                                                 |
|   |  Feb 2013 | Stipendiat der Karin Abt-Straubinger-Stiftung für eine Sprachschule an der technischen Staatsuniversität in Irkutsk, Sibirien |
|   |      2012 | Träger des Deutschlandstipendiums                    |
|   |      2012 | Stipendiat des DAAD go east! Programms an der Staatlichen Landesuniversität Russlands in Moskau |
|   |      2010 | Stipendiat des »Europäischen Chorforums für junge Komponisten« Ochsenhausen, in diesem Rahmen Aufführung eigener Werke durch das Orpheus Vokalensemble |
|   | 1998–2007 | Theodor-Heuss-Gymnasium Aalen, Preis der Schule und Preis der DPG (Deutsche Physikalische Gesellschaft) |
|---+-----------+------------------------------------------------------|

*** Musikalisches Schaffen
|---+-----------+------------------------------------------------------|
|   |       <r> | <52>                                                 |
|   |  Aug 2013 | Konzert im Dom zu Schweidnitz (Polen)                |
|   |  Jan 2013 | Leiter der Musikschule Obertürkheim Stuttgart www.musikschule-obertuerkheim.de |
|   |      2010 | Mehrere Uraufführungen eigener Kompositionen durch den Aalener Oratorienchor |
|   | 2008–2012 | Kirchenchorleiter und Organist der Marienwallfahrtskirche alen-Unterkochen |
|---+-----------+------------------------------------------------------|

*** Sonstige Tätigkeiten
|---+-----------+------------------------------------------------------|
|   |       <r> | <52>                                                 |
|   |  Feb 2013 | Mitglied im Doktorandennetzwerk des AMPF (Arbeitskreis Musikpädagogische Forschung) |
|   | seit 2012 | Mitglied im MinD-Hochschulnetzwerk www.mhn.mensa.de  |
|   |      2011 | Praktikum beim Landesgymnasium für Hochbegabte Schwäbisch Gmünd |
|   |      2011 | Abschlussprüfung in Chorleitung, Orchesterleitung und in Musiktheorie |
|   |      2010 | Studienreise nach Toulouse, Dolmetscher für die Studenten um sich in die frz. Kultur integrieren zu können |
|   |      2008 | Studienreise nach Italien um die dortige Orgellandschaft kennenzulernen |
|---+-----------+------------------------------------------------------|

*** Sprachen
|---+----------+------------------------------------------------------|
|   |          | <52>                                                 |
|   | English  | Sehr gut in Wort und Schrift                         |
|   | Español  | Sehr gut in Wort und Schrift                         |
|   | Français | Sehr gut in Wort, gut in Schrift                     |
|   | Русский  | Grundlegende Kenntnisse (A2)                         |
|---+----------+------------------------------------------------------|

*** Computer
|---+----------+------------------------------------------------------|
|   |          | <52>                                                 |
|   | R, SPSS  | Erfahrung in quantitativer Forschung                 |
|   | Python   | Grundlegende Programmierkenntnisse                   |
|   | C        | Einführung an Harvard (über edx.org)                 |
|   | Elisp    | Emacs                                                |
|   | ILIAS    | Serverinstallation und -benutzung                    |
|   | Org-mode | Projektplanungs- und Protokollierungserfahrung       |
|   | LaTeX    | Textpublikationserfahrungen                          |
|   | Lilypond | Auszeichnungssprache für professionellen Notensatz   |
|---+----------+------------------------------------------------------|

* Adaptives Testen
[2013-10-28 Mon]
** Einleitung
Beim adaptiven Testen werden die Möglichkeiten einer computerbasierten
Testung in erweitertem Maße ausgenutzt.  Es wird innerhalb der Testung
auf Grund der bereits beantworteten Fragen auf den Fähigkeitsgrad des
Probanden geschätzt, um ihm als nächstes eine Frage zu geben, die
diesen reflektiert.

#+BEGIN_SRC ditaa :file images/adaptiveditaa.png :exports results
+---------+   +--------+   +---------------+
|  Item-  |-->| Modell |-->|   Schätzung   |
| antwort |   +--------+   | nächstes Item |
+---------+                +-------+-------+
    ^                              |
    |                              |
    +------------------------------+
#+END_SRC

#+RESULTS:
[[file:images/adaptiveditaa.png]]

** Kriterien
Wie genau die Frage ausgewählt wird hängt vornehmlich von den Zielen
der Testung und den Nebenwirkungen, die man ggf. vermeiden möchte, ab.
So wäre eine Frage, die der Proband mit einer Chance von 50% lösen
kann ideal im Sinne eines Informationsgewinnes über den Probanden, da
die Entropie maximal ist.

Entropie stellt nicht nur Chaos, sondern auch Informationsdichte nach
Shannon dar (vgl. [[http://de.wikipedia.org/wiki/Entropie_%28Informationstheorie%29][Wikipedia]]).  Die folgende Formel ergiebt die
Entropie eines Ereignisses. Hierbei ist $n$ die Anzahl der
Möglichkeiten und $p_i$ die jeweiligen Wahrscheinlichkeiten dieser
(sich ausschließender) Möglichkeiten.  $$ H = - \sum_{i=1}^n p_i \cdot
\log_2{p_i} $$

Betrachten wir ein binäres System, also nur Fragen die entweder
vollständig falsch oder vollständig richtig beantwortet werden können,
so haben wir $n = 2$ und $p_2 = 1 - p_1$.  $$ H_2 = - p \cdot \log_2{p} -
(1 - p) \cdot \log_2(1 - p) $$

Hieraus ergiebt sich, dass der Informationsgewinn bei $p = .5$ (im
binären) maximal ist, da die Funktion symmetrisch ist und bei $p = 0$
auf beiden Seiten $0$ ergibt.  $$ H_{max} = - 0.5 \cdot \log_2{0.5} - 0.5
\cdot \log_2{0.5} = 1 \mathrm{bit} $$

Hierbei ist $bit$ die übliche Einheit des Informationsgehaltes, auf
Grund des $\log_2$. Es kann also ein Informationsgehalt von 8 bit mit
Hilfe eines bytes dargestellt werden, sprich mit 8 Nullen oder Einsen
(z.B. 10101010).

#+BEGIN_SRC R :results output graphics :file images/entropie.png :exports results
x = (0:100)/100
y = -x*log(x,2)-(1-x)*log(1-x,2)
plot(x,y,type="l",xlab=expression(Lösungswahrscheinlichkeit),ylab=expression("Entropie in bit"),  main="Entropieverteilung")
#+END_SRC

#+RESULTS:
[[file:images/entropie.png]]

Mit der Formel für tatsächliche und für maximale Entropie kann die
Redundanz ausgerechnet werden, welche in einer Testkonstruktion als
Indikator für das Potential der Verbesserung durch ein adaptives
Testverfahren benutzt werden kann.  $$ R = H_{max} - H $$

Somit kann im binären ein Test im Idealfall um $R$ Fragen verkleinert
werden, ohne an Informationen einzubüßen.  Befinden sich im Test auch
Fragen, die nicht binär sind, so verändert sich die Situation ein
wenig.  Die Entropie wird stets maximal bei gleichen
Wahrscheinlichkeiten.  Gibt es nun bei einer Frage z.B. 0, 1, 2 oder 3
Punkte gibt es eine maximale Entropie von $H_{max} = - \log_2{0.25} =
2 \mathrm{bit}$. Somit kann man einen Test auch um $R/2$ solcher
Fragen ohne Informationsverlust verkleinern.

*** Informationsgehalt in KoMus
Im August diesen Jahres habe ich mit Teilen des KoMus-Testes für
musikalische Kompetenz eine empirische Studie durchgeführt.  Der
KoMus-Test liegt in einem nicht adaptiven Format vor.

Interessant ist nun, die Überlegung, wieviel der Test von einer
Überführung in einen adaptiven Test profitieren würde.

**** Durchschnittlicher Schüler
Der Simplizität halber können wir annehmen, dass ein Schüler genau die
durchschnittlichen Lösungswahrscheinlichkeiten für ein Item aufweist.
So muss man nur die Entropie mit den klassischen Itemschwierigkeiten
der Items berechnen: $$ H = \sum_{i=1}^n(- P_i \cdot \log_2{P_i} - (1 - P_i)
\cdot \log_2(1 - P_i)) $$

Hierbei ist $P_i$ die klassische Itemschwierigkeit des Items $i$ und
$n$ die Anzahl der Items.  Ferner nimmt diese Formel nur dichotome
Items an.

**** Vierdimensional
Die Entropie ist jedoch eigentlich noch niedriger, wenn man die
Schwierigkeiten adaptiv berechnet.  Es handelt sich aber immernoch um
einen nicht adaptiven Test mit fester Reihenfolge. Es wird nur eine
spezialform der Entropie, die bedingte Entropie, benutzt
(vgl. [[http://de.wikipedia.org/wiki/Bedingte_Entropie][Wikipedia]]).

Um die bedingte Lösungschance und somit auch die bedingte Entropie zu
berechnen, werden alle Items der selben Dimension einer logistisch
binären Regression verwendet um die individuell Itemschwierigkeit
eines Items der gleichen Dimension zu berechnen.  $$ H =
\sum_{D=1}^4(\sum_{i=1}^{n(D)}(- R \cdot \log_2{R}) - (1 - R) \cdot
\log_2(1- R))$$ $$ R = Reg_2(P_{ivD}|\sum_{m=1}^{i(D)-1}P_{vmD}) $$

Hierbei ist $Reg_2(a|b)$ die binärlogistische Regression mit der AV
$a$ und den UV $b$.

**** n-Dimensional
Das obige Modell nimmt jedoch an, dass die einzelnen Dimensionen nicht
korrelieren und somit Itemantworten einer Dimension keine Information
(also Entropiesenkung) über andere Dimensionen zulassen.

Dies ist aber eine nicht notwendige Einschränkung, die die Berechnung
nicht nur weniger effektiv, sondern auch komplizierter macht.

Berücksichtigen wir unabhängig von der Dimensionszugehörigkeit einfach
alle bereits beantworteten Items, die einen signifikanten Einfluss auf
die Frage haben, haben wir im Endeffekt ein n-dimensionales Modell,
wobei $n$ die Anzahl der Fragen ist.  $$ H = \sum_{i=1}^n(- R \cdot
\log_2{R} - (1 - R) \cdot \log_2(1 - R)) $$ $$ R =
Reg_2(P_{iv}|\sum_{m=1}^{i-1}P_{vm}) $$

***** Probleme
Mögliche Probleme dieser Methode sind fehlende Datensätze, da die
binärlogistische Regression normalerweise alle Fälle ausschließt, die
auch nur bei einem der UV keinen Wert hat.  Durch den
maßgeschneiderten Testweg ist es bei einem Itempool, der nicht
komplett erschöpft wird, unmöglich diese Regression so durchzuführen.
Dementsprechend müssen andere Methoden gewählt werden, um mit
fehlenden Daten umzugehen.

Eine Möglichkeit wäre, nicht vorhandene Antworten in den Datensätzen
zu simulieren.  Diese Simulation würde von den Items die am sichersten
geschätzt werden können zu den Items, die schwer geschätzt werden
können stattfinden.

Der Grund hierfür liegt daran, dass bei einer sehr sicheren Schätzung
der Entropiegehalt sich nur wenig ändert, aber es gleich viel mehr
Personen gibt, die seriös für schwerere Schätzungen verwendet werden
können, was diese Schätzungen erleichtert.

Die Simulation sollte nach jeder Testung durchgeführt werden, um die
Simulation mehr und mehr zu verbessern.  Das dieses Verfahren direkt
einen Nutzen erbringt, sollte daran sichtbar sein, dass die Lösungen
der alten Datensätze immer besser retrospektiv vorhergesagt werden
können und somit davon außgegangen werden kann, dass auch bei
aktuellen Testungen die Schätzungen besser sind und somit effektiver
gemessen werden kann.

** Umsetzung
Die Umsetzung wurde mit R bewerkstelligt. Hier traten auch schnell
Probleme auf.  So wurde die Rechenzeit bei etwas komplizierteren
Modellen sehr lang, was natürlich auch an meinem Computer liegt.
Nichts desto trotz ergaben sich Situationen, in denen der Computer 5
Tage lang rechnen hätte müssen.

In anderen Situationen wurde das komplette RAM des Computers
aufgezehrt usw.

*** Programmierung
**** Initialisierung
Für alle nachfolgenden Berechnungen habe ich immer dieses Skript
benutzt, um grundlegende Dinge, wie Funktionen, die an vielen Stellen
benötigt werden, die Daten usw. bereitgestellt werden.  Ferner werden,
wo möglich, Berechnungen mit dieser Initialisierung parallelisiert.

#+NAME: statistic
#+BEGIN_SRC R :exports code :results output :noweb yes
require(MASS)
library(foreach)
library(doMC)
# number of cores to use
registerDoMC(4)

calculationtime = proc.time()
komus = read.table("_data/komus/data-komus-bin.csv",header=TRUE) # Perhaps breaks, it was in another format.
test = data.frame(read.table("_data/komus/data-komus-revised.csv",header=TRUE, sep=','))
pcitems = array(which(sapply(test, max) > 1))
pcitems.temp = pcitems
test[pcitems] = lapply(test[pcitems],factor)

# functions
FUN.infoMC = function(x)
{
    return(rowSums(-x*log(x+0.0000000001, 2)))
}

FUN.infoMC.IND = function(x)
{
    return(sum(-x*log(x+0.0000000001, 2)))
}

FUN.info = function(x)
{
    return(-x*log(x+0.0000000001, 2)-(1-x)*log(1-x+0.0000000001, 2))
}

FUN.Odds = function(x)
{
    if (length(pcitems.temp) > 0)
    {
        x[-pcitems.temp] = lapply(x[-pcitems.temp], predict, type="response")
        x[pcitems.temp] = lapply(x[pcitems.temp], predict, type="probs")
    } else
    {
        x = lapply(x, predict, type="response")
    }
    return(x)
}

FUN.Odds.IND = function(x,y)
{
    x[-pcitems.temp] = lapply(x[-pcitems.temp], predict, data = test[y,], type="response")
    x[pcitems.temp] = lapply(x[pcitems.temp], predict, data = test[y,], type="probs")
    return(x)
}

FUN.info.temp = function(x)
{
    if (length(pcitems.temp) > 0)
    {
        x[pcitems.temp] = lapply(x[pcitems.temp], FUN.infoMC)
        x[-pcitems.temp] =lapply(x[-pcitems.temp], FUN.info)
    } else   
    {
        x =lapply(x, FUN.info)
    }
    x = simplify2array(x)
    return(x)
}

FUN.info.temp.IND = function(x)
{
    if (length(pcitems.temp) > 0)
    {
        x[pcitems.temp] = lapply(x[pcitems.temp], FUN.infoMC.IND)
        x[-pcitems.temp] =lapply(x[-pcitems.temp], FUN.info)
    } else
    {
        x =lapply(x, FUN.info)
    }
    x = simplify2array(x)
    return(x)
}

FUN.EntroMC = function(funpcitems.temp, fundata, funmod)
{
    if (length(funpcitems.temp) == 1)
    {
        odds = lapply(funmod, predict, fundata, type="response")
        info.temp = odds
        info.temp = lapply(odds, FUN.info)
        info.temp = simplify2array(info.temp)
    } else
    {
        funpcitems.temp = funpcitems.temp[2:length(funpcitems.temp)]
        odds = funmod
        odds[-funpcitems.temp] = lapply(funmod[-funpcitems.temp], predict, fundata, type="response")
        odds[funpcitems.temp] = lapply(funmod[funpcitems.temp], predict, fundata, type="probs")
        info.temp = odds
        info.temp[funpcitems.temp] = lapply(odds[funpcitems.temp], FUN.infoMC.IND)
        info.temp[-funpcitems.temp] = lapply(odds[-funpcitems.temp], FUN.info)
        info.temp = simplify2array(info.temp)
    }
    return(info.temp)
}

odds = NULL
fit = NULL
modell = NULL
summe = data.frame()

# number of items and persons to consider in this calculation
items = length(test)
persons = length(test[,1])

info = NULL
info.rest = data.frame(matrix(ncol = 1, nrow = items+1))
info.rest.SD = data.frame(matrix(ncol = 1, nrow = items+1))
names(info.rest) = 'kill'
names(info.rest.SD) = 'kill'

entropie = data.frame(matrix(ncol = 1, nrow = items+1))
entropie.SD = data.frame(matrix(ncol = 1, nrow = items+1))
names(entropie) = 'kill'
names(entropie.SD) = 'kill'
#+END_SRC

#+RESULTS: statistic
: Loading required package: MASS
: Error in library(foreach) : there is no package called 'foreach'
: Error in library(doMC) : there is no package called 'doMC'
: Error: could not find function "registerDoMC"

**** Nichtadaptiv
***** Unbedingte und bedingte info in normaler Reihenfolge
Dieser verhältnismäßig simple Code berechnet die info über die
klassische Itemschwierigkeit und die info über die durch
binär-logistische Regressionen vorhergesagte Itemschwierigkeit in der
ursprünglichen Reihenfolge.  Zudem wird bei zweiter Berechnung noch
angegeben, wie viel entropie.rest nach jeder Antwort noch zu erwarten
ist.
#+NAME: statistic1
#+BEGIN_SRC R :exports code :results output :noweb yes
modell = NULL
pcitems.temp = pcitems.temp[pcitems.temp <= items]

if (1 %in% pcitems.temp)
{
    modell[[1]] = polr(reformulate('1', names(test[1])), data = test)
} else
{
    modell[[1]] = glm(reformulate('1', names(test[1])), data = test, family = "binomial"(link=logit))
}

for (i in 2:items)
{
    if (i %in% pcitems.temp)
    {
        modell[[i]] = polr(reformulate(names(test[1:i-1]), names(test[i])), data = test)
    } else
    {
        modell[[i]] = glm(reformulate(names(test[1:i-1]), names(test[i])), data = test, family = "binomial"(link=logit))
    }
}

fit = modell
<<fit>>
odds = FUN.Odds(fit)
info.temp = FUN.info.temp(odds)

### Without relations ###
fit = lapply(fit, update, ~ 1)
odds2 = FUN.Odds(fit)
info.temp2 = FUN.info.temp(odds2)
pcitems.temp = pcitems
query = NULL
rest.temp = NULL

for (i in 1:items)
{
    info.temp3 = NULL
    fit3 = NULL
    
    if (i == length(test))
    {
        rest.temp[[i]] = rest.temp[[1]]*0
    } else
    {
        query = 1:i
        pcitems.temp = which(names(test[-query]) %in% names(test[pcitems]))
        
        for (j in 1:length(test[-query]))
        {
            if (j %in% pcitems.temp)
            {
                fit3[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
            } else
            {
                fit3[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = "binomial"(link=logit))
            }
        }
        
        <<fit>>
        odds3 = FUN.Odds(fit3)
        info.temp3 = FUN.info.temp(odds3)
        rest.temp[[i]] = rowSums(info.temp3)
    }
}

rest.temp = simplify2array(rest.temp)
info.rest$bedunsort = c(0,colMeans(rest.temp))
info.rest.SD$bedunsort = c(0,apply(rest.temp, 2, sd))
entropie.SD.temp = sd(info.temp[,1])

for (i in 2:length(info.temp[1,]))
{
    entropie.SD.temp[i] = sd(rowSums(info.temp[,1:i]))
}

entropie.SD$bedunsort = c(0,entropie.SD.temp)
entropie$bedunsort = c(0,colMeans(info.temp))
entropie$unbedunsort = c(0,colMeans(info.temp2))
entropie$unbedsort = c(0,sort(colMeans(info.temp2), decreasing =TRUE))
info.temp2 = data.frame(info.temp2)
names(info.temp2) = names(test[1:length(info.temp2)])
entropie
info.rest
#+END_SRC

***** Bedingte, sortierte info
Hier werden die Items schlicht nach dem durchschnittlichen infogehalt
sortiert, bevor die bedingte info mit Regressionen berechnet wird.
Dies verbessert die resultierende Kurve schon um einiges, der
infogewinn ist so tendenziell am Anfang weit höher als am Ende, trotz
dass gleich viel info innerhalb des kompletten Durchlaufes ermittelt
wurde.
#+NAME: statistic2
#+BEGIN_SRC R :exports code :results output :noweb yes
modell = NULL
odds = NULL
fit = NULL

############## sortierte Reihenfolge
for (i in 1:items)
{
    if (i %in% pcitems.temp)
    {
        modell[[i]] = polr(reformulate('1', names(test[i])), data = test)
    } else
    {
        modell[[i]] = glm(reformulate('1', names(test[i])), data = test, family = "binomial"(link=logit))
    }
}

odds = FUN.Odds(modell)
info.temp = FUN.info.temp(odds)
info.temp = data.frame(info.temp)
names(info.temp) = names(test[1:length(info.temp)])
komus2 = test[c(names(sort(colMeans(info.temp), decreasing=TRUE)))]
#########

names(sort(colMeans(info.temp), decreasing=TRUE))
pcitems.temp.alt = pcitems.temp
pcitems.temp.alt
pcitems.temp = which(names(komus2) %in% names(test[pcitems.temp.alt]))
modell = NULL
fit = NULL
odds = NULL

if (1 %in% pcitems.temp)
{
    modell[[1]] = polr(reformulate('1', names(komus2[1])), data = komus2)
} else
{
    modell[[1]] = glm(reformulate('1', names(komus2[1])), data = komus2, family = "binomial"(link=logit))
}

for (i in 2:items)
{
    if (i %in% pcitems.temp)
    {
        modell[[i]] = polr(reformulate(names(komus2[1:i-1]), names(komus2[i])), data = komus2)
    } else {
        modell[[i]] = glm(reformulate(names(komus2[1:i-1]), names(komus2[i])), data = komus2, family = "binomial"(link=logit))
    }
}

fit = modell
<<fit>>
odds = FUN.Odds(fit)
#odds[-pcitems.temp] = lapply(fit[-pcitems.temp], predict, type="response")
#odds[pcitems.temp] = lapply(fit[pcitems.temp], predict, type="probs")

#info.temp = fit
info.temp = FUN.info.temp(odds)
#info.temp[pcitems.temp] = lapply(odds[pcitems.temp], FUN.infoMC)
#info.temp[-pcitems.temp] =lapply(odds[-pcitems.temp], FUN.info)
#info.temp = simplify2array(info.temp)

entropie.SD.temp = sd(info.temp[,1])

for (i in 2:length(info.temp[1,]))
{
    entropie.SD.temp[i] = sd(rowSums(info.temp[,1:i]))
}

entropie.SD$sortbed = c(0,entropie.SD.temp)
entropie$sortbed = c(0,colMeans(info.temp))
pcitems.temp = pcitems.temp.alt
#+END_SRC

***** Durchschnittlich bedingtsortierte info
Dieses Verfahren ist bereits weit rechenintensiver, es wird
nacheinander das Item ausgewählt, welches durchschnittlich die info am
meisten senkt.  Es wird also nach der Erfassung eines Items dieses
miteinbezogen für kommende Regressionen.  Insgesamt ist dies aber noch
nicht individualisiert und dementsprechen nicht adaptiv.
#+NAME: statistic3
#+BEGIN_SRC R :exports code :results output :noweb yes
query = NULL
modell = NULL
rest.temp = NULL
pcitems = pcitems.temp
############## sortierte Reihenfolge
for (i in 1:length(test))
{
    if (i %in% pcitems.temp)
    {
        fit[[i]] = polr(reformulate('1', names(test[i])), data = test)
    } else
    {
        fit[[i]] = glm(reformulate('1', names(test[i])), data = test, family = "binomial"(link=logit))
    }
}
odds = FUN.Odds(fit)
info.temp = FUN.info.temp(odds)
query = which(names(test[which(colMeans(info.temp) == max(colMeans(info.temp)))[1]]) == names(test))[1]
query
modell[[1]] = fit[[query]]

for (i in 2:items) {
    info.temp = NULL
    fit = NULL
    pcitems.temp = which(names(test[-query]) %in% names(test[pcitems]))
    
    for (j in 1:length(test[-query]))
    {
        if (j %in% pcitems.temp)
        {
            fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
        } else
        {
            fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = "binomial"(link=logit))
        }
    }
    <<fit>>
    odds = FUN.Odds(fit)
    info.temp = FUN.info.temp(odds)
    rest.temp[[i-1]] = rowSums(info.temp)
    query = c(query, which(names(test[-query][which(colMeans(info.temp) == max(colMeans(info.temp)))[1]]) == names(test))[1])
    modell[[i]] = fit[[which(colMeans(info.temp) == max(colMeans(info.temp)))[1]]]
}

if (length(test) == items)
{
    rest.temp[[items]] = rest.temp[[1]]*0
} else
{
    fit = NULL
    pcitems.temp = which(names(test[-query]) %in% names(test[pcitems]))
    
    for (j in 1:length(test[-query]))
    {
        if (j %in% pcitems.temp)
        {
            fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
        } else
        {
            fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = "binomial"(link=logit))
        }
    }
    <<fit>>
    odds = FUN.Odds(fit)
    info.temp = FUN.info.temp(odds)
    rest.temp[[items]] = rowSums(info.temp)
}

pcitems.temp = which(query %in% pcitems)
rest.temp = simplify2array(rest.temp)
odds = FUN.Odds(modell)
info.temp = FUN.info.temp(odds)
entropie.SD.temp = sd(info.temp[,1])

for (i in 2:length(info.temp[1,]))
{
    entropie.SD.temp[i] = sd(rowSums(info.temp[,1:i]))
}

entropie.SD$durchschbedsort = c(0,entropie.SD.temp)
entropie$durchschbedsort = c(0,colMeans(info.temp))
info.rest$durchschbedsort = c(0,colMeans(rest.temp))
info.rest.SD$durchschbedsort = c(0,apply(rest.temp, 2, sd))
#+END_SRC

**** Adaptiv
***** Individuellbedingtsortierte info
Hier wird das zuletzt genannte Verfahren individualisiert, was den
Rechenaufwand in diesem Fall 319 mal höher macht.  Das Ergebniss ist
jedoch bereits ein echt adaptiver Test.  Somit ist die infokurve nun
auch viel stärker gekrümmt (hat also eine größere zweite Ableitung).
Somit kann unter kleinem Informationsverlust der Test stark verkürzt
werden.

Ideal wäre ein Itempool, der nicht komplett erschöpft wird in einer
Testung. Somit könnte man berechnen, wie lang ein nichtadaptiver im
Vergleich zu einem gleichpräzisen adaptiven Test ist.
#+NAME: statistic4
#+BEGIN_SRC R :exports code :results output :noweb yes
## initializing
infoall = NULL
odds = NULL
rest.temp = NULL
query = NULL
modell = NULL
rest.temp = NULL
pcitems.temp = pcitems
fit = NULL

## first item
for (i in 1:length(test))
{
    if (i %in% pcitems.temp)
    {
        fit[[i]] = polr(reformulate('1', names(test[i])), data = test)
    } else
    {
        fit[[i]] = glm(reformulate('1', names(test[i])), data = test, family = "binomial"(link=logit))
    }
}

odds = fit
odds[-pcitems.temp] = lapply(fit[-pcitems.temp], predict, test[1,], type="response")
odds[pcitems.temp] = lapply(fit[pcitems.temp], predict, test[1,], type="probs")
info.temp = FUN.info.temp.IND(odds)
query = which(names(test[which(info.temp == max(info.temp))[1]]) == names(test))[1]
modell[[1]] = fit[[query]]
queryinit = query
fit = NULL

## multicorecalculation for every person
infoall = simplify2array(foreach(k=1:persons) %dopar%
{
    query = queryinit
    entropie.rest = NULL
    
    for (i in 2:items)
    {
        odds = NULL
        info.temp = NULL
        fit = NULL
        pcitems.temp = c(0,which(names(test[-query]) %in% names(test[pcitems])))
        
        for (j in 1:length(test[-query]))
        {
            if (j %in% pcitems.temp)
            {
                fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
            } else
            {
                fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = "binomial"(link=logit))
            }
        }
        
        <<fit>>
        odds = fit
        info.temp = FUN.EntroMC(pcitems.temp,test[k,], fit)
        rest.temp[i-1] = sum(info.temp) #rest of entropie before this item
        query = c(query, which(names(test[-query][which(info.temp == max(info.temp))[1]]) == names(test))[1])
        modell[[i]] = fit[[which(info.temp == max(info.temp))[1]]]
    }
    
    ## calculation of last rest entropie
    if (length(test) == items)
    {
        rest.temp[items] = 0
    } else
    {
        fit = NULL
        pcitems.temp = 0
        pcitems.temp = c(0,which(names(test[-query]) %in% names(test[pcitems])))
        
        for (j in 1:length(test[-query]))
        {
            if (j %in% pcitems.temp)
            {
                fit[[j]] = polr(reformulate(names(test[query]), names(test[-query][j])), data = test)
            } else
            {
                fit[[j]] = glm(reformulate(names(test[query]), names(test[-query][j])), data = test, family = "binomial"(link=logit))
            }
        }
        <<fit>>
        info.temp = FUN.EntroMC(pcitems.temp,test[k,], fit)
        rest.temp[items] = sum(info.temp)
    }
    
    ## calculation of the choosen modell
    pcitems.temp = c(0,which(query %in% pcitems))
    info.temp = FUN.EntroMC(pcitems.temp,test[k,], modell)
    return(c(info.temp, rest.temp))
})

rest.temp = (infoall[(items+1):(items*2),])
infoall = infoall[1:items,]
entropie.SD.temp = sd(infoall[1,])

for (i in 2:length(infoall[,1]))
{
    entropie.SD.temp[i] = sd(colSums(infoall[1:i,]))
}

entropie.SD$indivbedsort = c(0,entropie.SD.temp)
entropie$indivbedsort = c(0,rowMeans(infoall))
info.rest$indivbedsort = c(0,rowMeans(rest.temp))
info.rest.SD$indivbedsort = c(0,apply(rest.temp, 1, sd))
pcitems.temp = pcitems
#+END_SRC

***** Individuellbedingtsortierte info mit Trennschärfe
Ein nicht gut gelungener Versuch, nicht nur die info als
Auswahlkriterium zu nehmen. Dies ist deswegen sinnvoll, da Items
vorstellbar sind mit hoher info, die aber mit dem Test wenig zu tun
haben (z.B. eine Frage nach der Schuhgröße hat vermutlich eine sehr
hohe info, hat aber vermutlich wenig mit musikalischer Kompetenz zu
tun).  Somit macht das bisherige Verfahren die Annahme, dass der
Itempool sehr gut konstruiert ist.  Dementsprechend kann man das
bisherige Verfahren sicher nicht als robust bezeichnen.
#+NAME: statistic5
#+BEGIN_SRC R :exports code :results output :noweb yes
infoall = NULL
odds = NULL
beta = NULL
rest.temp = NULL
info.temp = NULL
fit = NULL

if (!exists("information"))
{
    information = simplify2array(foreach(m=1:length(komus)) %dopar%
    {
        for (n in 1:(length(komus)-1))
        {
            beta[[n]] = glm(reformulate(names(komus[m]), names(komus[-m][n])), data = komus, family = "binomial"(link=logit))
        }
        odds = simplify2array(lapply(beta, predict, type="response"))
        chancetemp = unlist(lapply(komus[m],mean))
        info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2))
        information = sum(colMeans(info.temp)) + (-chancetemp*log(chancetemp,2)-(1-chancetemp)*log(1-chancetemp,2))
        return(information)
    })
    information = -(information - sum(-colMeans(komus)*log(colMeans(komus),2)-(1-colMeans(komus))*log(1-colMeans(komus),2)))
}

for (j in 1:length(komus))
{
    fit[[j]] = glm(reformulate('1', names(komus[j])), data = komus, family = "binomial"(link=logit))
}

<<fit>>
odds = simplify2array(lapply(fit, predict, komus[1,], type="response"))
info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2)) + (information)
queryinit = which(names(komus[which((info.temp) == max((info.temp)))[1]]) == names(komus))[1]
modell[[1]] = fit[[which((info.temp) == max((info.temp)))[1]]]

infoall = simplify2array(foreach(k=1:persons) %dopar%
{
    query = queryinit
    
    for (i in 2:items)
    {
        info.temp = NULL
        fit = NULL
        
        for (j in 1:length(komus[-query]))
        {
            fit[[j]] = glm(reformulate(names(komus[query]), names(komus[-query][j])), data = komus, family = "binomial"(link=logit))
        }
        
        <<fit>>
        ## TODO stimmt das so?
        odds = simplify2array(lapply(fit, predict, komus[k,], type="response"))
        rest.temp[i-1] = sum(-odds*log(odds,2)-(1-odds)*log(1-odds,2)) 
        info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2)) + (information[-query]*(1 - (length(query)+1)/items))
        query = c(query, which(names(komus[-query][which((info.temp) == max((info.temp)))[1]]) == names(komus))[1])
        modell[[i]] = fit[[which((info.temp) == max((info.temp)))[1]]]
    }
    
    if (length(komus) == items)
    {
        rest.temp[items] = 0
    } else
    {
        fit = NULL
        
        for (j in 1:length(komus[-query]))
        {
            fit[[j]] = glm(reformulate(names(komus[query]), names(komus[-query][j])), data = komus, family = "binomial"(link=logit))
        }
        <<fit>>
        odds = simplify2array(lapply(fit, predict, komus[k,], type="response"))
        rest.temp[length(query)] = sum(-odds*log(odds,2)-(1-odds)*log(1-odds,2))
    }
    
    odds = simplify2array(lapply(modell, predict, komus[k,], type="response"))
    info.temp = (-odds*log(odds,2)-(1-odds)*log(1-odds,2))
    return(c(info.temp, rest.temp))
})

rest.temp = (infoall[(items+1):(items*2),])
infoall = infoall[1:items,]
entropie.SD.temp = sd(infoall[1,])

for (i in 2:length(infoall[,1]))
{
    entropie.SD.temp[i] = sd(colSums(infoall[1:i,]))
}

entropie.SD$indivbedsorttrenn = c(0,entropie.SD.temp )
entropie$indivbedsorttrenn = c(0,rowMeans(infoall))
info.rest$indivbedsorttrenn = c(0,rowMeans(rest.temp))
info.rest.SD$indivbedsorttrenn = c(0,apply(rest.temp,1 ,sd))
#+END_SRC

***** Individuellbedingtsortierte info mit Prädiktion
Hier wird nun die info rekursiv berechnet.  Es wird nicht nur
geschaut, welches Item die meiste info besitzt, sondern es werden für
jedes Item alle Antwortmöglichkeiten simuliert und mit dieser
Simulation die verbleibende info im gesamten Test errechnet, diese mit
der Chance der simulierten Antwort gewichtet und aufaddiert mit den
gewichteten anderen Antwortmöglichkeiten.

Dieses Modell umgeht also das Problem der vorherigen beiden.  Es ist
sehr robust, weil immer auch berechnet wird, wie sehr sich das
auserwählte Item mit all seinen Antwortmöglichkeiten auf die gesamte
entropie.rest auswirkt.  Dies ist eine mächtigere Form der
Trennschärfe, weil sie nicht starr, sondern antwortmusterspezifisch
ist.

Dieses Modell bringt die rechnerischen Anforderungen auf ein neues
Niveau, sie werden nochmals ungefähr 30 mal höher.  Als Konsequenz
daraus habe ich hier eine Datenbank mit implementiert, die einerseits
bereits berechnetes speichert um mir wiederholte Arbeit zu ersparen
und andererseits stets schaut, ob Frage-Antwort-Kombinationen bereits
bei anderen Schülern vorgekommen ist, um mit Hilfe dieses Wissens hin
und wieder einzelne Rechnungen zu ersparen.

Zunächst könnte man denken, dass es bei rund 50 binären Items $2^{50}$
Möglichkeiten der Antwortmuster gibt, was die Datenbank als sinnlos
erscheinen lässt.  Jedoch muss bedacht werden, dass die Antwort
Reihenfolge in der aktuellen Regression keine Rolle
spielt. Beantwortet man Item a, b, und c richtig und bekommt daraufhin
Item c, so würde man dies genauso bekommen, wenn man b, c und dann
erst a richtig beantwortet, was die Sinnhaftigkeit der Datenbank
deutlich steigert.  Zudem werden manche Antwortmuster und manche Items
gehäuft vorkommen, weil sie entweder besonders qualitativ, oder
besonders normal sind.  Im Moment fangen beispielsweise alle Schüler
mit dem gleichen, maximal informativen Item an, weil noch keine
Vorinformation über die Schüler vorhanden ist.

#+NAME: statistic6
#+BEGIN_SRC R :exports code :results output :noweb yes

##### Calculate the benefit of an adaptive test with non-adaptive data.
#
#  Use data.dat to predict answers of items.
#  Return a database (database.dat) of proposed items for each individual.
#  Draw a curve of the linear and adaptive test.
#  Afterwards, search.sh should be run to update the database.
# 
#####

pcitems.temp = pcitems
fit = NULL
modell = NULL
infoall = matrix(nrow=persons, ncol=items)

# Return a fitted modell, which considers polytomus and dichotomus items.
FUN.modelliteration = function(fun.fit.formula, fun.pcitems, fun.iterator)
{
    if (fun.iterator %in% fun.pcitems)
    {
        fun.tempfit = polr(fun.fit.formula, data = test)
    } else
    {
        fun.tempfit = glm(fun.fit.formula, data = test, family = "binomial")
    }
    return(fun.tempfit)
}

# Binary search in database.dat for each column to match already calculated predictions.
FUN.binarysearch = function(query)
{
    if (!exists("database"))
    {
        return(0)
    }
    
    lower = 1
    upper = length(database[, 1])
    current = upper
    
    while (upper >= lower)
    {
        current = round((lower+upper) / 2)
        
        for (u in 1:length(query))
        {
            if (answers.sorted[u] > database[current, u])
            {
                lower = current + 1
                break
            } else if (answers.sorted[u] < database[current, u])
            {
                upper = current - 1
                break
            } else if (u == length(query))
            {
                if (!database[current, (u+1)])
                {
                    return(current)
                } else
                {
                    upper = current - 1
                    break
                }
            }
        }
    }
    
    return(0)
}

# Linear search in new data, if binary search fails.
# This is a lot slower and should be only used, if unsorted database is small.
FUN.unsortedsearch = function(query)
{
    if (exists("newdata"))
    {
        for (m in 1:length(newdata[, 1]))
        {
            for (u in 1:length(query))
            {
                if (answers.sorted[u] != newdata[m, u])
                {
                    break
                }
                
                if (!newdata[m,(length(query)+3)] && u == length(query))
                {
                    return(m)
                }
            }
        }
    }
    return(0)
}

# Return the modell for the first item.
for (i in 1:length(test)) 
{
    fit.formula = reformulate('1', names(test[i]))
    fit[[i]] = FUN.modelliteration(fit.formula, pcitems, i)
}

odds = fit
odds[-pcitems.temp] = lapply(fit[-pcitems], predict, test[1,], type="response")
odds[pcitems.temp] = lapply(fit[pcitems], predict, test[1,], type="probs")
info.temp = FUN.info.temp.IND(odds)
query = which(names(test[which(info.temp == max(info.temp))[1]]) == names(test))[1]
modell[[1]] = fit[[query]]
queryinit = query

# Calculate item predictions for each person for all remaining items.
for (k in 1:persons)
{
    query = queryinit
    entropie.rest = NULL
    rest.temp = NULL
    rest.temp2 = NULL
    
    calcu = 0
    calcutime = proc.time()
    answers = vector(length = (length(test)+2))
    answers[1] = query[1] + as.numeric(as.character(test[k, query[1]]))/100
    
    # Check existing databases.
    if (file.exists('_data/komus/sorted-database.dat'))
    {
        database = read.table('_data/komus/sorted-database.dat')
    }
    
    if (file.exists('_data/komus/newdata.dat'))
    {
        newdata = read.table('_data/komus/newdata.dat')
    }
    
    # Calculate next item.
    for (i in 2:items)
    {
        odds = NULL
        info.temp = NULL
        info.temp2 = NULL
        fit = NULL
        fit2 = NULL
        pcitems.temp = c(0, which(names(test[-query]) %in% names(test[pcitems])))
        answers.sorted = sort(answers[1:length(query)])
        
        # Search in database if pattern of answers already exists.
        found = FUN.binarysearch(query)
        
        if (found)
        {
            lq = length(query)
            rest.temp[i-1] = database[found, (lq+2)]
            found = database[found, (lq+3)]
            query = c(query, found)
        } else
        {
            found = FUN.unsortedsearch(query)
            if (found)
            {
                lq = length(query)
                rest.temp[i-1] = newdata[found, (lq+1)]
                found = newdata[found, (lq+2)]
                query = c(query, found)
            }
        }
        
        # If pattern isn't found, calculate the new one.
        if (!found)
        {
            calcu = calcu+1
            isgood = NULL
            
            # Create a modell for each remaining item with all answers as predictors.
            for (q in 1:length(test[-query]))
            {
                fit.formula = reformulate(names(test[query]), names(test[-query][q]))
                fit[[q]] = FUN.modelliteration(fit.formula, pcitems.temp, q)
            }
            
            # Calculate the entropie of these items and choose good ones.
            info.temp = FUN.EntroMC(pcitems.temp, test[k, ], fit)
            isgood = info.temp >= (max(info.temp)[1] * 0.8)
            
            # Multicore calculation.
            Liste = foreach(j=1:length(test[-query])) %dopar%
            {
                # Give the probability of each answer possibility.
                if (j %in% pcitems.temp)
                {
                    chance = predict(fit[[j]], test[k,], type="probs")
                } else
                {
                    chance = predict(fit[[j]], test[k,], type="response")
                    chance[2] = 1-chance[1]
                }
                
                rest.temp2 = NULL
                
                # Predict the entropie with simulated answers of good items.
                if (length(test[-query]) > 1 && isgood[j])
                {
                    pcitems.temp2 = c(0, which(names(test[-query][-j]) %in% names(test[pcitems])))
                    
                    # Calculate the fitted modell of each item.
                    for (n in 1:length(test[-query][-j]))
                    {
                        fit2.formula = reformulate(names(c(test[query], test[-query][j])), names(test[-query][-j][n]))
                        fit2[[n]] = FUN.modelliteration(fit2.formula, pcitems.temp2, n)
                    }
                    
                    # Calculate the reduction of entropie multiplied by the chance of these answers.
                    tempdata = test[k, ]
                    
                    if (j %in% pcitems.temp)
                    {
                        for (s in 1:length(chance))
                        {
                            tempdata[-query][j] = factor(s-1)
                            info.temp = FUN.EntroMC(pcitems.temp2, tempdata, fit2)*chance[s]
                            rest.temp2[s] = sum(info.temp)
                        }
                        rest.temp2 = sum(rest.temp2)
                    } else
                    {
                        tempdata[-query][j] = 1
                        info.temp = FUN.EntroMC(pcitems.temp2, tempdata, fit2)*chance[1]
                        rest.temp2 = sum(info.temp)
                        tempdata[-query][j] = 0
                        info.temp = FUN.EntroMC(pcitems.temp2, tempdata, fit2)*chance[2]
                        rest.temp2[2] = sum(info.temp)
                        rest.temp2 = sum(rest.temp2)
                    }
                    
                    fit2 = NULL
                } else
                {
                    # If the item is the last one, take it.
                    # If the item isn't good, don't take it.
                    if (isgood[j])
                    {
                        rest.temp2 = 0
                        
                    } else 
                    {
                        rest.temp2 = 55555555
                    }
                }
                
                return(rest.temp2)
            }
            
            # Save the best modell, add the proposed item to the list of questions.
            rest.temp2 = simplify2array(Liste)
            query = c(query, which(names(test[-query][which(rest.temp2 == min(rest.temp2))[1]]) == names(test))[1])
            modell[[i]] = fit[[which(rest.temp2 == min(rest.temp2))[1]]]
            <<fit>>
            odds = fit
            info.temp = FUN.EntroMC(pcitems.temp, test[k,], fit)
            rest.temp[i-1] = sum(info.temp)
        }
        
        answers[i] = query[i] + as.numeric(as.character(test[k, query[i]]))/100
        
        # Save the new calculated pattern to the database.
        if (!found) 
        {
            temp = answers
            temp[1:(i-1)] = sort(answers[1:(i-1)])
            temp[i] = rest.temp[i-1]
            temp[i+1] = query[i]
            write(temp, file='_data/komus/newdata.dat', append=TRUE, ncolumns=length(answers))
        }
        
        plot(rest.temp, type='l', col=rgb(0, 0.7, 0.7))
    }
    
    # Calculate the last remaining entropie (which may be non-nil if not all items are answered).
    if (length(test) == items)
    {
        rest.temp[items] = 0
    } else
    {
        fit = NULL
        pcitems.temp = c(0, which(names(test[-query]) %in% names(test[pcitems])))
        
        for (j in 1:length(test[-query]))
        {
            fit.formula = reformulate(names(test[query]), names(test[-query][j]))
            fit[[j]] = FUN.modelliteration(fit.formula, pcitems.temp, j)
        }
        
        <<fit>>
        info.temp = FUN.EntroMC(pcitems.temp, test[k, ], fit)
        rest.temp[items] = sum(info.temp)
    }
    
    # Save the number of new patterns and seconds for this person.
    pcitems.temp = c(0, which(query %in% pcitems))
    
    if (calcu)
    {
        temp = vector(length=(length(test)+2))
        temp[2] = k
        temp[3] = (proc.time() - calcutime)[3]
        temp[4] = calcu
        write(temp, file='_data/komus/newdata.dat', append=TRUE, ncolumns=length(answers))
    }
    
    infoall[k,] = rest.temp
}

# Clean-up.
## restt = (infoall[(items+1):(items*2)ü,])
## infoall = infoall[1:items,]
## 
## entropie.SD.temp = sd(infoall[1,])
## for (i in 2:length(infoall[,1])) {
##     entropie.SD.temp[i] = sd(colSums(infoall[1:i,]))
## }
## 
## entropie.SD$indivbedsortpred = c(0, entropie.SD.temp)
## entropie$indivbedsortpred = c(0, rowMeans(infoall))
info.rest$indivbedsortpred = c(0, colMeans(infoall))
info.rest.SD$indivbedsortpred = c(0, apply(infoall, 2, sd))

pcitems.temp = pcitems
# Give some output for debugging.
## entropie
## entropie.SD
info.rest
info.rest.SD


#+END_SRC

**** Schlussberechnungen
Hier werden lediglich ein paar Aufräumarbeiten in den Daten noch
erledigt, um diese dann gut zeichnen zu können.
#+NAME: statisticend
#+BEGIN_SRC R :exports code :results output :noweb yes
if (names(entropie[1]) == 'kill')
{
    entropie = entropie[-1]
}

if (names(entropie.SD[1]) == 'kill')
{
    entropie.SD = entropie.SD[-1]
}

if (names(info.rest[1]) == 'kill')
{
    info.rest = info.rest[-1]
    info.rest.SD = info.rest.SD[-1]
}

for (i in 1:length(entropie[1,]))
{
    for (j in 1:length(entropie[,1]))
    {
        summe[j,i] = sum(entropie[1:j,i])
    }
}

fit = NULL
pcitems.temp = pcitems

for (i in 1:length(test))
{
    if (i %in% pcitems.temp)
    {
        fit[[i]] = polr(reformulate('1', names(test[i])), data = test)
    } else
    {
        fit[[i]] = glm(reformulate('1', names(test[i])), data = test, family = "binomial"(link=logit))
    }
}

odds = fit
odds[-pcitems.temp] = lapply(fit[-pcitems.temp], predict, test[1,], type="response")
odds[pcitems.temp] = lapply(fit[pcitems.temp], predict, test[1,], type="probs")
info.temp = FUN.info.temp.IND(odds)
info.rest[1,] = sum(info.temp)

names(summe) = names(entropie)

if (exists("benchmark"))
{
    benchmark = array(c(benchmark,(proc.time() - calculationtime)[3]))
} else
{
    benchmark = (proc.time() - calculationtime)[3]
}
#+END_SRC

**** Formel für die Modellanpassung
Hier kann noch bestimmt werden, ob die binärlogistischen Regressionen
noch schlechte Items verwerfen, oder einfach mit allen rechnen.
Änderungen, die hier gemacht werden, werden automatisch im gesamten
Code angepasst, da dieser Teil mit noweb-syntax eingebunden ist.

Aus statistischer Sicht ist es natürlich viel besser, wenn schlechte
Items noch verworfen und noch Interaktionen hinzugefügt werden.  Was
hier aber dagegen spricht, ist die dadurch resultierende
Berechnungsdauer.  So sind selbst die einfacheren obigen Modell auch
nach Stunden nicht fertig.
#+NAME: fit
#+BEGIN_SRC R :exports code
#fit = lapply(fit, step, trace = 0)
#fit = lapply(fit, step, ~.^2, trace = 0)
#+END_SRC

**** Benchmark
#+BEGIN_SRC R :noweb yes :results output graphics :file images/benchmark.png :exports code
plot(benchmark, type="l", col=rgb(0,0,0), ann=F)
title(xlab="Durchlauf")
title(ylab="Dauer")
#+END_SRC

#+RESULTS:
[[file:images/benchmark.png]]

**** infografik
Hier ist noch ein letztes kleines Bisschen an Code, welches die
derzeit kalkulierten Ergebnisse in eine Grafik packt.  Zudem werden
eine Legende generiert und die Berechnungsdauer angegeben.
#+NAME: grafik
#+BEGIN_SRC R :noweb yes :results output graphics :file images/entropie2.png :exports code
farbe = NULL
farbeSD = NULL
for (j in 1:(length(summe[1,])))
{
    r = runif(1,0.1,0.9)
    g = runif(1,0.1,0.9)
    b = runif(1,0.1,0.9)
    farbe[j] = rgb(r^1.2, g^1.2, b^1.2)
    farbeSD[j] = rgb(sqrt(r), sqrt(g), sqrt(b))
}

plot(0:(length(test)), type="l", col=rgb(0,0,0), ann=F)

for (i in 1:(length(summe[1,])))
{
    lines(summe[,i], col=farbe[i])
    
    if (dim(entropie.SD[names(entropie.SD) == names(summe[i])])[2] != 0)
    {
        lines(summe[,i]+entropie.SD[names(summe[i])],lty = 4, col=farbeSD[i])
        lines(summe[,i]-entropie.SD[names(summe[i])],lty = 4, col=farbeSD[i])
    }
    
    if (dim(info.rest[names(info.rest) == names(summe[i])])[2] != 0)
    {
        lines(info.rest[names(summe[i])], col=farbe[i])
        lines(info.rest[names(summe[i])]+info.rest.SD[names(summe[i])],lty = 4, col=farbeSD[i])
        lines(info.rest[names(summe[i])]-info.rest.SD[names(summe[i])],lty = 4, col=farbeSD[i])
    }
}

title(xlab="Anzahl der beantworteten Fragen")
title(ylab="Entropie in bit")
legend(length(test)/4, length(test), c(names(summe), round(benchmark[length(benchmark)])), cex=0.9, col=c(farbe, rgb(1,1,1)), lty=1)
#+END_SRC

#+RESULTS: grafik
[[file:images/entropie2.png]]

* Magit!
[2014-01-24 Fri]
** Einleitung
Da ich nun schon seit ein paar Monaten mit Hilfe von Git meine
Homepage versioniere und auf Github publiziere, befand ich die Zeit
reif dafür, mich genauer mit Git auseinanderzusetzen.

** Git
Git wurde von Linus Torvalds für die Versionierung des Linuxkernels
programmiert.  Da die Funktionalität sehr umfangreich ist, erfreute
sich Git schnell großer Beliebtheit.

*** Funktionsweise
Es wird in dem Repository, also in dem Verzeichnis, in dem man seine
Dateien aufbewahrt werden, welche versioniert werden sollen
(i.d.R. Quellcode) ein .git Verzeichnis erstellt, welches zunächst
einer Kopie aller Dateien entspricht.  Daraufhin werden Änderungen in
den Staging-Bereich gebracht:
#+BEGIN_SRC sh :exports code
git add -A
#+END_SRC
Sobald man dort alle Änderungen hinzugefügt hat, welche man
versionieren möchte, macht man einen Commit, welcher einen Text
enthält, welcher beschreibt, welche Änderungen vorgenommen werden:
#+BEGIN_SRC sh :exports code
git commit -m 'bugfix'
#+END_SRC
Somit ist nun die Änderung gespeichert und kann mit dem vorherigen
Stand verglichen werden, neue Änderungen gemacht werden, oder die
Änderungen zu einem Remote-Repository übertragen werden:
#+BEGIN_SRC sh :exports code
git push origin master
#+END_SRC

#+BEGIN_SRC ditaa :file images/git.png :exports result
 +------------------+ 
 |Arbeitsverzeichnis+<-------+
 +----------------+-+        |
                  |          |
 +---git add------+          |
 |                           |
 |  +-----+                  |
 +->|Stage|-----+            |
    +-----+     |            |
                |            |
 +---git commit-+            |
 |                           |
 |  +----------+             |
 +->|Repository+-+           |
    +----------+ |           |
                 |           |
 +---git push----+           |
 |                           |
 |  +------+                 |
 +->|Remote+--git pull-------+
    +------+
#+END_SRC

#+RESULTS:
[[file:images/git.png]]

** Magit
Mit Magit kann man auf komfortable Weise diffs, also Vergleiche
zwischen zwei Versionen ansehen.  Ferner hat man den Staging-Bereich,
und den Bereich der Änderungen, die noch nicht im Staging-Bereich sind
im Blick, was den großen Vorteil bietet, dass nur die Änderungen, die
eine logische Einheit bilden, in den Staging-Bereich aufgenommen
werden können, um die diese zu commiten.  Andere Änderungen werden
erst in darauffolgenden Commits mit entsprechender Beschreibung
publiziert.

Dies sieht dann Beispielsweise folgendermaßen aus:
#+BEGIN_SRC html :exports results
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!-- Created by htmlize-1.47 in css mode. -->
<html>
  <head>
    <title>*magit: knupfer.github.io*</title>
    <style type="text/css">
    <!--
      body {
        color: #ffffff;
        background-color: #000000;
      }
      .ATTRLIST {
        /* (:weight ultra-bold :foreground "#05ffff") */
        color: #05ffff;
      }
      .ATTRLIST-1 {
        /* (:weight ultra-bold :foreground "#e07fef") */
        color: #e07fef;
      }
      .ATTRLIST-2 {
        /* (:weight ultra-bold :foreground "#f0cf05") */
        color: #f0cf05;
      }
      .ATTRLIST-3 {
        /* (:weight ultra-bold :foreground "#ee5555") */
        color: #ee5555;
      }
      .ATTRLIST-4 {
        /* (:weight ultra-bold :foreground "#ffffff") */
        color: #ffffff;
      }
      .ATTRLIST-5 {
        /* (:weight ultra-bold :foreground "#00ff00") */
        color: #00ff00;
      }
      .magit-branch {
        /* magit-branch */
        color: #e5e5e5;
        background-color: #333333;
      }
      .magit-item-highlight {
        /* magit-item-highlight */
        background-color: #4a708b;
      }
      .magit-log-message {
      }
      .magit-log-sha1 {
        /* magit-log-sha1 */
        color: #ff6347;
      }
      .magit-section-title {
        /* magit-section-title */
        color: #e5e5e5;
        background-color: #333333;
      }

      a {
        color: inherit;
        background-color: inherit;
        font: inherit;
        text-decoration: inherit;
      }
      a:hover {
        text-decoration: underline;
      }
    -->
    </style>
  </head>
  <body>
    <pre>
Local:    <span class="magit-branch">master</span> ~/git/knupfer.github.io/
Remote:   <span class="magit-branch">master</span> @ origin (https://github.com/knupfer/knupfer.github.io.git)
Head:     <span class="magit-log-sha1">1f236ae</span> Bugfix in Ditaa graphic

<span class="magit-section-title">Untracked files:</span>
    _org/Computer/.#2014-01-24-magit.org

<span class="magit-section-title">Unpushed commits:</span>
<span class="magit-log-sha1">1f236ae</span> <span class="magit-log-message">Bugfix in Ditaa graphic</span>
<span class="magit-log-sha1"><span class="magit-item-highlight">da99ab0</span></span><span class="magit-item-highlight"> </span><span class="magit-log-message"><span class="magit-item-highlight">Correct connections in Ditaa graphic</span></span><span class="magit-item-highlight">
</span><span class="magit-log-sha1">9943f8e</span> <span class="magit-log-message">Clean files with *~</span>

</pre>
  </body>
</html>
#+END_SRC

* Emacs Org-Mode
[2013-10-13 Sun]
** Einleitung
#+name: paper
#+begin_src lilypond :exports none
\version "2.16.0"
#(ly:set-option 'resolution 140)

\paper{
    indent=0\mm
    line-width=90\mm
    oddFooterMarkup=##f
    oddHeaderMarkup=##f
    bookTitleMarkup=##f
    scoreTitleMarkup=##f
}
#+end_src

Org-Mode stellt eine der interessantesten Entdeckungen dar, die ich
die letzten Monate gemacht habe. Org-Mode ist ein Modus, also eine
Erweiterung, für den Texteditor Emacs. Org-Mode dient, wie der Name
bereits impliziert, der Organisation.

** Outliner
Org-Mode bietet sehr elaborierte Outlinefähigkeiten. Hierarchien
werden mit einer unterschiedlichen Anzahl an Sternen gezeigt.

#+BEGIN_SRC org
 * Erste Überschrift
 Text
 
 ** Unterüberschrift
 Text
 
 *** Unterunterüberschrift
 Text

 * Zweite Überschrift
#+END_SRC

Jede Unterüberschrift kann nur genau ein Elternteil (Überschrift)
haben, jedoch kann jede Überschrift beliebig viele Kinder
(Unterüberschriften) haben. Dies verhält sich also genau so, wie man
es in einer geschriebenen Arbeit erwarten würde.

Um sich einen Überblick zu verschaffen und in seinem Dokument
effizient zu navigieren, kann man einzelne Überschriften mit Hilfe der
Tabulatortaste falten (ausblenden). Das obige Beispiel sieht gefaltet
wie folgt aus:

#+BEGIN_SRC org
 * Erste Überschrift...
 * Zweite Überschrift
#+END_SRC

Zudem kann man ganze Hierarchiebäume verschieben und auch befördern
bzw. degradieren.

** Todo-Listen
Es können Todo-Listen einfach erstellt werden und mit Tastenkürzel
verändert werden, beispielsweise als erledigt markiert werden und
automatisch einen Zeitstempel hinzufügen.

**** TODO offener Punkt
**** DONE abgeschlossener Punkt.
   CLOSED: [2013-10-13 Sun 12:31]

Und folgendermaßen sehen die beiden Punkte im Quelltext aus:

#+BEGIN_SRC org
 *** TODO offener Punkt
 *** DONE abgeschlossener Punkt.
    CLOSED: [2013-10-13 Sun 12:31]
#+END_SRC

*** Agendas
Mit Hilfe dieser Todo-Listen können sich Projektpläne erstellen
lassen, ferner können die Todos mit einer Frist versehen werden und
die ganze Liste an Fristen dann in einen Terminkalender
exportieren. Erledigte Punkte können per Tastenkürzel in eine andere
Datei archiviert werden.

Ferner lassen sich per Befehl auch alle Todos eines Projektes, welches
aus mehreren .org Dateien bestehen kann anzeigen und exportieren.

** Export
Da Org-Mode alles in simplen Textdateien speichert ist zwar einerseits
die Kompatibilität und die Portabilität sehr hoch (man könnte
Org-Mode-Dateien sogar mit Notepad erstellen), aber das Aussehen doch
sehr limitiert.

Dementsprechend kann man in gängige, ästhetischere Formate
exportieren:
1. LaTeX
   1. Entweder nur in die .tex Datei
   2. Oder direkt in ein pdf, wobei die Datei mehrfach prozessiert wird und alle anderen dabei entstehenden Dateien automatisch gelöscht werden
2. Odt
3. Html
   1. Mit Head
   2. Ohne Head

Der Export ist so hervorragend, dass die resultierenden Pdfs direkt
gedruckt werden können. Nach bedarf können einzelne Exportoptionen
über Variablen verändert werden.

*** LaTeX
LaTeX-Formeln können direkt in das Html exportiert werden. Hierfür
muss keine besondere Notation bedacht werden, es ist die gleiche. Nur
die $-Zeichen der LaTeX-Formelumgebung müssen mit der Formel anliegend
notiert werden, um Verwechslungen mit dem Währungssymbol zu
vermeiden. Als Beispiel der Laplace-Operator für multidimensionale
Differentialrechnung: $$ \Delta=\sum_{k=1}^n \frac{\partial^2}{\partial x_k^2} $$

Im Quelltext sieht dies folgendermaßen aus:

#+BEGIN_SRC latex -r
$\Delta=\sum_{k=1}^n \frac{\partial^2} (ref:formel)
                          {\partial x_k^2}$
#+END_SRC

Diese komplette Homepage ist mit Org-Modes Html-Export gemacht. Ich
exportiere nur den Body und lass das resultierende Html durch Jekyll
automatisch prozessieren, danach ein push nach GitHub, und fertig!

*** Sourceblöcke
Man kann jedoch nicht nur LaTeX-Code exportieren oder verwenden,
sondern auch nahezu jede Programmier- und Markupsprache. So gab es
weiter oben bereits mehrfach Codebeispiele in Org-Mode. Diese gibt man
einfach folgendermaßen an:

#+BEGIN_SRC org
,#+BEGIN_SRC org
,* Testüberschrift
,#+END_SRC
#+END_SRC

**** Python
***** Fibonacci
Hier nun ein Beispiel eines Programmcodes in Python, welcher beim
Export direkt ausgeführt wird und das Ergebnis automatisch in eine
Tabelle überführt wird. Es wurde keine Zahl händisch in die Tabelle
eingetragen.

#+BEGIN_SRC python :exports both
    # Fibonacci-Reihe
    x = 5
    y = 5
    fib = [[0] * x for i in range(y)]
    fib[0][0] = 1
    for i in range(x * y):
        for j in range (1, 3):
            fib[i % y][i // y] += fib[(i - j) % y][(i - j) // y]
    return fib   
#+END_SRC

#+RESULTS:

Ferner ist es möglich, die Ergebnisse eines Sourceblocks für den Input
eines anderen Sourceblocks zu verwenden. Dementsprechend können,
ähnlich zu Shellscripten, Pipes zwischen verschiedenen
Programmiersprachen gebaut werden. Das Ergebnis kann an jeder
beliebigen Stelle im Dokument durch einen Platzhalter eingefügt
werden, wodurch es möglich ist, Tabellen und Grafiken automatisch
immer auf den aktuellen Datenstand darzustellen.

Man kann sich auch vorstellen, dass backupscripte die Datei
automatisch bei jedem Export sichert, oder aber für eine Website
nachbereitet. Es ist vor allem Erfreulich, dass man sich dadurch das
mühselige übertragen von Daten in Tabellen (beispielsweise in LaTeX)
ersparen kann.

***** Zahlensystemkonverter
#+BEGIN_SRC python :exports both

# Zahlensystemkonverter
Konvertierungszahl = 1000
Zahlenraum = 20
Zahlensystem = [[0] for i in range(Zahlenraum - 1)]
Resultat = [[''] * 2 for i in range(Zahlenraum - 1)]

for i in range(2, Zahlenraum + 1):
    Stelle = 0
    Zahlensystem[i-2][0] = Konvertierungszahl
    while Zahlensystem[i-2][0] > 0:
        Stelle += 1
        Zahlensystem[i-2].insert(1,chr(ord('0')
                + (Zahlensystem[i - 2][0] % i)))
        Zahlensystem[i-2][0] //= i
    Zahlensystem[i - 2][0] = i    

for i in range(Zahlenraum - 1):
    Resultat[i][0] = Zahlensystem[i][0]
    for j in range(1, len(Zahlensystem[i])):
        Resultat[i][1] += str(Zahlensystem[i][j])

return Resultat

#+END_SRC

#+RESULTS:
|  2 | 1111101000 |
|  3 |    1101001 |
|  4 |      33220 |
|  5 |      13000 |
|  6 |       4344 |
|  7 |       2626 |
|  8 |       1750 |
|  9 |       1331 |
| 10 |       1000 |
| 11 |        82: |
| 12 |        6;4 |
| 13 |        5;< |
| 14 |        516 |
| 15 |        46: |
| 16 |        3>8 |
| 17 |        37> |
| 18 |        31: |
| 19 |        2>< |
| 20 |        2:0 |

**** Lilypond
Ferner kann direkt Sourcecode von Lilypond verwendet werden. Somit
können musikwissenschaftliche oder -pädagogische Bücher mit gemischtem
Anteil an Noten und Text ohne Probleme erstellt werden. Ferner aus den
gleichen Quellen Html-Seiten exportiert werden.

Kombiniert mit den programmatischen Möglichkeiten, die sich durch die
verschiedenen Sourcecodeblöcken ergeben -- das Ergebnis des einen
Sourcecodeblocks kann direkt für die Verwendung eines anderen
Sourcecodeblocks verwendet werden -- lassen sich viele Dinge
automatisieren, oder als Variable verwenden. So bietet es sich an, den
Header und das Layout von Lilypond als Variable zu speichern, welche
dann stets, bei gleichem Layout, verwendet werden kann.

***** Moderne Notation
#+begin_src lilypond :file images/ly-einfach.png :noweb yes :exports both
<<paper>>
  
\relative c' {
    r2 e |
    e f2. e8 d c2 |
    c4 e f g2 f4 d e4. d8 d2 c4 |
    d2 
}

\addlyrics {
    Et a -- scen -- _ _ _ dit in coe -- _ _ _ _ _ _ _ lum. 
}
#+end_src

#+RESULTS:
[[file:images/ly-einfach.png]]

***** Historische Notation
#+begin_src lilypond :file images/lily-beispiel.png :noweb yes :exports both
<<paper>>
 
\language "deutsch"

\score {
    <<
        \new Voice = mel \relative c' { 
            \set Score.defaultBarType = "-"
            \clef "petrucci-c3"
            \autoBeamOff
            \override NoteHead #'style = #'petrucci
            \override Staff.TimeSignature #'style = #'neomensural
            
            \time 2/2
            
            r2 e |
            e f2. e8 d c2 |
            c4 e f g2 f4 d e4. d8 d2 c4 |
            d2 
        }
        
        \addlyrics { 
            Et a -- scen -- _ _ _ dit in coe -- _ _ _ _ _ _ _ lum. 
        }
    >>
    
    \layout {
        \context {
            \Staff
            \consists "Custos_engraver"
            \override Custos #'style = #'mensural
        }
    }
}

#+end_src

#+RESULTS:
[[file:images/lily-beispiel.png]]

* Emacs whitespace.el
[2014-01-26 Sun]
** Einleitung
Als Anfänger im Programmieren habe ich mir die Frage gestellt, wie ich
mir das Programmierenlernen so effektiv und komfortabel wie möglich
gestalten kann.

Es stellte sich zunächst die Frage, ob man nun mit Spaces oder Tabs
indenten sollte.  Nachdem ich einige Artikel im Internet gelesen habe
und auch über unkonventionelle Lösungen wie elastic tabs nachgedacht
habe, habe ich mich für Leerzeichen entschieden.

** Vergleich mit anderen Methoden
*** farbige Pipes und kein farbiger Hintergrund
** Vorteile
*** In aktuellen Emacs-Versionen inbegriffen
*** verschiedene Farben für verschiedene Indentation-Tiefe
** Nachteile
*** Bugs
**** langsam für restlichen Whitespace, wenn der Point darübergeht...
**** in Teilen inkompatibel mit org-mode indentation-mode


* Neo Layout
[2013-10-21 Mon]
** Einleitung
Vor zwei Jahren bin ich mehr oder minder über das alternative
Tastaturlayout Neo gestolpert und mich seither intensiv mit Ergonomie
und der Geschichte des konventionellen QWERTY-Layouts
auseinandergesetzt.

** Konventionelles Tastaturlayout
Die heutige übliche Tastaturbelegung geht auf das 19. Jahrhundert
zurück. Ziel war, häufige Buchstabenfolgen auf der vormaligen
alphabetischen Anordnung der Schreibmaschinentasten möglichst weit
auseinanderzulegen, um ein Verkanten der Hämmer zu vermeiden.

Somit wurde auf Grund einer technischen Einschränkung die Tasten
explizit so angeordnet, dass sie für den Menschen möglichst schlecht
liegen. Diesen technischen Einschränkungen unterliegen wir heutzutage
natürlich nicht mehr, jedoch hat sich auf Grund von der bereits
etablierten Konvention das bestehende Layout (bis auf regionale
Varianten) nicht mehr geändert.

Eine weitere Besonderheit ist, dass man auf der englischen Variante
"Typewriter" komplett auf der obersten Reihe schreiben kann, was wohl
ein Easteregg ist.

Der horizontale Tastenversatz liegt auch an den Hämmern, die Platz für
ihre Traktur benötigten. Jedoch ist dieser Versatz nicht
tippfreundlich, da man um höhere oder niedrigere Tasten zu erreichen
man seinen Finger nicht nur strecken oder einziehen, sonder auch
verdrehen muss.

** Alternative Tastaturlayouts
Mit der Zeit bildeten sich Alternativen heraus, die aber allesamt
nicht sehr populär sind, was daran liegen mag, da:
1. Viele kein Zehnfingersystem benutzen
2. QWERTY der ubiquitäre Standard ist
3. Die Nachteile nicht bekannt sind
4. Die alternativen Layouts unbekannt sind

*** Dvorak
Das bekannteste und auch älteste Reformlayout ist das Dvorak-Layout,
welches 1930 entwickelt wurde.

**** Ergebnisse
Hierfür wurden verschiedene Studien bezüglich Ergonomie und Effizienz
durchgeführt. Wichtige, auch in aktuelleren Reformbelegungen präsente,
Ergebnisse waren, dass:
- Die Finger möglichst kurze Bewegungen durchführen sollten
- Dementsprechend sollten möglichst häufige Buchstaben in der
  Ruheposition der Finger sich befinden
- Der gleiche Finger nicht wiederholt werden sollte
- Im besonderen Maße sollten Sprünge des gleichen Fingers von der
  obern auf die untere Reihe, oder andersrum vermieden werden
- Dass sich die Hände im optimalfall abwechseln
- Bigramme, also häufige Buchstabenkombinationen, sollten weit
  entfernt liegen
- Die Hände sollten ungefähr gleich viel zu tun haben
- Die starken Finger sollten häufiger verwendet werden als die
  schwachen, um Ermüdungserscheinungen zu vermeiden

**** Umsetzung
Diese Ergebnisse konnten jedoch nur händisch umgesetzt werden, da
entsprechende Computer zur Berechnung und Optimierung nicht zur
Verfügung standen.

Nichts desto trotz wurde eine auch heute noch beliebte und
ergonomische Tastaturbelegung erstellt, die in modernen
Betriebssystemen als Standard einstellbar ist.

*** Ristome
*** Neo
Diese Layout wurde 2004 entwickelt und speziell auf die deutsche
Sprache optimiert. Die Ergebnisse Dvoraks wurden hier verwendet, um
computergestützt das Layout zu optimieren. Jedoch waren die
Optimierungsmöglichkeiten selbst noch nicht ausgereift, was zu der
Entstehung von Neo2 führte.

**** Neo2
Hier wurde das gesamte Layout nochmals computergestützt deutlich
verbessert, so werden die Finger viel weniger bewegt. Beispielsweise
kann man 60% eines normalen deutschen Textes auf der Grundreihe
tippen.

Eine phantastische Besonderheit des Neolayouts, welche mich damals
dazu bewog, es zu lernen, ist, dass es noch zusätzliche Ebenen
besitzt. Dies bedeutet, dass man über Spezialtasten die Belegung des
Layouts ändern kann (wie z.B. bei Shift, welches für Großbuchstaben
sorgt). So gibt es eine Ebene mit einem Zahlen- und Navigationsblock,
welche unglaublich praktisch ist. So kann ich, während ich diesen Text
hier schreibe, in der Grundposition ohne Probleme Zahlen eingeben, zum
letzten Wortanfang springen, zum Zeilenanfang springen, in beide
Richtungen Zeichen-, Wort-, Absatz- oder sogar Dokumentenweise
löschen. Eine andere Ebene beinhaltet typische Programmierzeichen,
eine weitere enthält griechische Buchstaben (für Formeln) und die
letzte enthält mathematisch Spezialzeichen (Summationszeichen etc.).

**** AdNW
Dieses Layout steht für „Aus der Neo-Welt“ und sieht sich selbst als
Nachfolger des Neo-Layoutes. Hier wurde das Layout noch weiter
verfeinert und nicht nur an deutschen, sondern auch an englischen
Texten ausgerichtet. Somit ist dieses Layout weit internationaler und
trotzdem auch in der deutschen Sprache dem Neo-Layout deutlich
überlegen.

Meines Wissens nach ist dies derzeit das beste und vielseitigste
Tastaturlayout. Es wurde von vielen auch schon unter dem Namen Neo3
vorgeschlagen, jedoch gibt es manch eine Unstimmigkeit zwischen den
Projekten. Parallel dazu wird an einem anderen Neo3 gearbeitet, was
sehr gut ist, da Neo bereits einen beachtlichen Bekanntheitsgrad
erlangt hat, so ist das Neo-Layout in jeder Linuxdistro als
Belegungsvariante des Deutschen auswählbar.

* Linuxdistributionen
[2013-10-09 Wed]
** Einleitung
Als ich vor ungefähr 2 Jahren auf Linux umgestiegen bin, stellte ich
mir die Frage welche Distribution ich nehmen sollte. Überwältigt von
der Vielfalt und zunächst nicht im Stande Desktopumgebungen (Xcfe,
Lxde, Gnome …) von Distributionen zu unterscheiden entschied ich mich
für Ubuntu.

Nach einer Eingewöhnungszeit wurde mir klar, dass Cannonical (die
Firma hinter Ubuntu) Interessen hatte, die mit den meinigen
unvereinbar waren. So wechselte ich zu Debian und stellte nach kurzer
Zeit die Repository auf Testing. Hier hatte ich mich nun auch an
verschiedene Desktopumgebungen rangemacht.

** Distributionen
Distributionen kann man durch mehrere Elemente charakterisieren:

- Packetmanager :: Sie sind für Update- und Installationsprozesse
                   verantwortlich.
- Desktopumgebung :: Diese stellt eine grafische Oberfläche und
     verschiedene Standardprogramme bereit, da diese besonders wichtig
     für die Usability sind, wird hier auch noch detailiert darauf
     eingegangen.
- Free vs. Nonfree :: Manche Distributionen enthalten (fast) nur freie
     Software (z.B. Debian) was zur Konsequenz hat, dass diese von
     Haus aus die meisten Videocodecs nicht unterstützen. Andere
     Distribution sind komfortabler (z.B. Mint), da sie alles
     Notwendige enthalten, um beispielsweise eine DVD oder
     Youtube-Videos zu sehen.
- Zielgruppe :: Hier gibt es große Unterschiede, vor allem Ubuntu wird
                immer Anfängern empfohlen, da vieles ohne
                Konfiguration funktioniert und technisches unter
                grafischen Elementen versteckt wird. Arch hingegen hat
                vor allem erfahrene Nutzer im Blick. Auf ein
                grafisches Verstecken wird bewusst verzichtet, um
                maximal präzise Konfiguartionen zu
                ermöglichen. Erfahrene Nutzer können terminalbasiert
                schneller arbeiten, ferner wird die CPU durch die
                fehlenden Extravaganzen geschont.
- Entwicklungsstrategie :: Hier wird zwischen Rolling- und
     Nonrolling-release unterschieden. Da dieses Thema für mich
     besonders interessant ist, wird im Folgenden darauf genauer
     eingegangen.

*** Nonrolling-release
Die meiste Software hat eine Nonrolling-release-Entwicklung. Sprich es
werden stabile Versionen eines Programmes publiziert, von denen man
dann Updates zur nächsten stabilen Version machen kann. Bei mancher
Software, wie z.B. bei vielen Distributionen ist hierfür eine
Neuinstallation notwendig.

**** Debian
Debian ist eine sehr alte Distribution mit sehr vielen
Entwicklern. Der Entwicklungsprozess ist sehr konservativ; es werden
ungefähr alle zwei Jahre neue Versionen publiziert, welche in der
gesamten Zwischenzeit keinen neuen Features, sondern nur
Sicherheitsupdates bekommen. Da diese Versionen außerordentlich stabil
sind werden diese meist für Server verwendet.

***** Repositorys
Die Repositorys von Debian sind weiterhin in Experimental, Unstable
und Testing unterteilt.

****** Experimental
Hier befinden sich Programme, die große Systemveränderungen benötigen
oder noch sehr unstabil laufen. Experimental ist kein vollständiges
Repository, sondern kann nur zusammen mit Unstable verwendet
werden. Generell sollte man dieses Repository meiden, außer man möchte
explizit bei der Debianentwicklung mitarbeiten.

****** Unstable
Unstable ist ein vollständiges Repository, welches verwendet werden
kann um sehr aktuelle Software zu installieren. Unstable ist ein
Rolling-release, Neuinstallationen sind also nurnoch notwendig, falls
einem die unstabile Software zu viele Probleme bereitet. Auch von
diesem Repository wird i.d.R. abgeraten.

****** Testing
Dieses Repository hat schon eine gewisse Stabilität erreicht, da hier
nur Software gelagert wird, die eine Zeit lang ohne Probleme in
Unstable war. Jedoch benutzt Testing kein volles
Rolling-release-Modell mehr, da immer ein halbes Jahr vor einer neuen
stabilen Debianversion es einen Featuerfreeze gibt, in dem die
Entwickler sich auf Fehlerbehandlung fokussieren.

***** Derivate
Wegen des Alters und der Größe von Debian entstanden Derivate, also
Modifikationen, welche sich vor allem im Bezug zur freien Software
unterscheiden.

****** Ubuntu
Die bekannteste Linuxdistro nennt sich Ubuntu, was so viel wie
Menschlichkeit bedeutet. Sie ist auf einfache Benutzung und somit auf
unerfahrene Benutzer hin optimiert.

******* Kritik
******** Kommerz
In der letzten Zeit wurde Ubuntu immer mehr kommerzialisiert. Auch
wenn die Benutzung weiterhin kostenlos ist, so werden nun Verhalten
der Benutzer protokolliert und an Amazon verkauft. Ferner strebt die
Distribution gewisse Inkompatibilitäten an, welche zu einer
Monopolstellung (vor allem auf dem Smartphone-Markt) führen sollen.

******** Stabilität
Weiterhin baut Ubuntu auf dem Unstable-release von Debian auf, ohne
jedoch selbst ein Rolling-release zu sein. Dies hat zur Konsequenz,
dass zwar aktuelle Software verwendet wird, jedoch diese aber nicht
neu gehalten wird, was aber gerade bei unstabiler Software besonders
wichtig ist.

****** Mint
Diese Distro ist ein Fork von Ubuntu. Sie hat auch eher unerfahrene
Benutzer im Blick, lässt jedoch mehr Freiheiten und
Konfigurierbarkeiten. Zudem setzt mint auf einen klassischen Desktop,
im Gegensatz zu Ubuntu.

******* Kritik
******** Codebase
Problematisch dürfte die Basis Ubuntu sein, da dieses immer
inkompatibler wird. Somit dürften die Modifikationen, die Mint
vornimmt immer schwieriger und monolithischer werden, was den
UNIX-Prinzipien widerspricht. Um dem entgegenzutreten wurde eine
Version von Mint erstellt, welche direkt auf Debian aufbaut ([[Mint LMDE][LMDE]]).

******** Unfrei
Ferner wird kritisiert, dass Mint standardmäßig eine Vielzahl an
unfreier Software installiert (was andererseits zu einer komfortablen
Nutzungserfahrung führt).

**** Fedora
Diese Distribution unterscheidet sich in mancherlei Hinsicht von
Debian. Sie hat einen anderen Packetmanager, ist (immer) modern und
hat eine große Firma im Rücken (Red Hat).

**** OpenSuse
Eine deutsche, weitverbreitete Distro, welche das freie Pendant zu
Suse darstellt. Sie hat also auch eine Firma im Rücken (welche
unliebsamen Einfluss nehmen kann).

*** Rolling-release
Bei Rolling-release-software hat der Anwender stets eine aktuelle
Version der Software und kann jederzeit auf den derzeitigen
Entwicklungsstand updaten. In aller Regel geschieht dies ohne
Neuinstallation. Da es hier aber keine klar definierten Versionen
gibt, sondern nur Snapshots, sind evtl. auch mehr Fehler oder Probleme
zu finden. Dementsprechend sind die meisten
Rolling-release-distributionen für erfahrene Nutzer konzipiert.

**** Arch
[[http://archlinux.org][Arch]] ist eine sehr berühmte Rolling-releas-distro, welche sich vor
allem an sehr erfahrene Nutzer wendet. Beispielsweise installiert der
Installer (welcher rein Terminalbasiert ist) nur ein minimales
System. Eine Desktopumgebung, Videocodecs und Büroprogramme etc. muss
selbst installiert werden. Dies hat den unschlagbaren Vorteil, dass
nur das installiert wird, was man auch wirklich möchte. Andererseits
werden unerfahrene Benutzer mit einem nicht funktionsfähigen System
konfrontiert. Um im selbstversuch zu lernen, werde ich vermutlich als
nächstes Arch installieren.

***** Aktualität
Eines der Hauptziele von Arch ist es, möglichst nahe der bleeding Edge
zu sein. Dies hat den Vorzug stets aktuelle Software zu besitzen und
nie wieder das System neu installieren zu müssen, jedoch auch den
Nachteil, dass das ganze System hin und wieder unstabil oder gar
defekt sein kann. Da die Community sehr aktiv ist, werden Fehler
schnell behoben; dennoch empfiehlt sich diese Distro nicht für Server
oder sensible Daten.

***** Manjaro
Hier werden versucht, die Vorzüge von Arch mit denen von Mint zu
kombinieren, dementsprechend nutzt auch Manjaro standardmäßig die von
Mint entwickelte klassische Desktopumgebung Cinnamon. Ferner sind
Videocodecs etc. bereits installiert. Leider büßt diese Distro an
Aktualität ein, da sie trotz der Codebase Updatepackages verteilt und
somit kein volles Rolling-release mehr ist.

**** Debian
Da Debian zwei Repositories hat, die ständig entwickelt werden, kann
Debian unter Verwendung dieser auch als Rolling-release-distribution
betrachtet werden. Da dies aber nicht der eigentlich konservativen
Philosophie Debians entspricht gibt es einige Forks, die auf diesen
Repositories aufbauen, nicht aber auf dieser Philosophie.

***** Derivate
****** Aptosid
Eine auf dem Unstable-repository aufbauende Distro.

****** Crunchbang
Ebenso auf Unstable bauend.

****** Mint LMDE
Auf Testing aufbauend, mit der Behauptung (oder Traum), ein
Rolling-release zu sein. Da aber das DE viele Probleme bereitet,
vertreibt Mint Updatepackages. Trotz dass die Idee eines
debianbasierten hervorragend ist, so ist doch die Umsetzung viel
schlechter als bei dem originalen Mint.

** Desktopumgebungen
Ein Betriebssystem besteht auf den ersten Blick vor allem aus
Programmen und einer grafischen Oberfläche (GUI). Diese Elemente sind
aber nicht eigentlicher Teil eines Linuxbetriebssystemes, sondern Teil
von Desktopumgebungen. Hier sieht man einen grundlegenden Unterschied
zwischen Linux und Windows besonders deutlich: Linux ist modular, und
nicht monolithisch.

*** LXde
Eine besonders ressourcenschonende Desktopumgebung, die vor allem für
ältere Rechner gedacht ist. Kritikpunkte sind die alte Software und
das sehr kleine Entwicklerteam, was ein verwaisen der Software
ermöglicht.

*** Xfce
Auch ressourcenschonend, jedoch weiter verbreitet, moderner und mit
größerem Entwicklungsteam.

*** Kde
Sehr verbreitet, großes Entwicklerteam, grafisch aufwendig und
dementsprechend langsam.

*** Gnome
Diese DE hat eine lange Entwicklungsgeschichte hinter sich und ist bei
vielen Distros der Standard. Jedoch löste das Gnome-Projekt auch eine
sehr große Kontroverse mit Gnome3 aus, welches sich vom damaligen
klassischen Desktopparadigma (und somit von vielen Nutzern) abwandte.

**** Mate
Ein Fork von Gnome2, welcher versucht, den alten Desktop zu pflegen
und mit Sicherheitsaktualisierungen zu versorgen.

**** Cinnamon
Ein Fork von Gnome3, welcher versucht, trotz Integration neuster
Softwaretechnologie immernoch eine klassische DE
bereitzustellen. Cinnamon ist die Standard-DE von Mint und von Manjaro
und erfreut sich großer Beliebtheit.

**** Unity
Ebenso ein Gnome-fork, welcher aber mit dem Original nurnoch sehr
wenig zu tun hat. Unity ist mit nahezu allen Distros außer Ubuntu
inkompatibel. Es strebt einen primitiven (leicht benutzbaren) Desktop
an, der gut auch auf Touch-screens und Tablets verwendet werden kann.


* Hello World!
[2013-10-06 Sun]
** Einleitung
Hallo, dies ist mein erster Post hier. Ich bin gespannt, ob ich mich
für das Bloggen begeistern kann. Gleichzeitig dient diese Homepage
dazu, gemeinschaftlich über Probleme nachzudenken. Diese können aus
ganz verschiedenen Bereichen kommen. Hier im Speziellen wird es sich
um Themen der Musikpädagogik, der Empirie, des Computers und der
Typography drehen.

*** Zu meiner Person
Ich bin derzeit Student der Schul- und Kirchenmusik und der
Physik. Ich plane nach dem Studium einen Doktor in empirischer
Musikpädagogik zu machen und beschäftige mich deswegen nun seit ein
paar Wochen mit dem Programmieren.  Angefangen habe ich mit Python;
als pädagogische Programmiersprache erschien mir dies besonders
Sinnvoll in anbetracht meiner mangelnden Erfahrung. Nach ein paar
ersten Progrämmchen verlor ich ein wenig die Motivation aufgrund von
fehlenden Problemstellungen.

Dann bin ich auf die Lernplattform [[http://www.edx.org][EdX]] gestoßen. Diese bietet
kostenlose MOOCs an, also Vorlesungen (meist vom MIT oder Harvard),
die den regulären Studenten auch angeboten werden. Dort bin ich nun in
CS50 eingeschrieben und besuche regelmäßig Vorlesungen zum
Programmieren und bearbeite auch die dortigen Aufgaben. So bin ich
erst zu C und dann zu PHP gekommen.

** Programmierung der Homepage
Die Grundlagen dieser durch [[http://www.jekyllrb.com][Jekyll]] compilierten Homepage haben mich
einige Tage gekostet, da ich noch kaum Erfahrung mit Markup-Sprachen
hatte. Ferner wollte ich einige Prozesse automatisieren und
verbessern, was mir manch einen Nerv geraubt hat.

*** Jekyll
Jekyll ist ein statischer Homepage generierer der vor allem bei
Programmieren sehr beliebt ist. Dies liegt an Verschiedenem.

**** Statische Homepages
Der große Vorteil an statischen Homepages ist, dass diese keine
Datenbank wie MySQL benötigen. Zudem benötigen sie kein PHP und sind
somit nicht nur sehr sicher, sondern auch auf jedem Web-Server
funktionsfähig. Weiterhin sind statische Homepages sehr performant.

**** GitHub
Ein weiterer fantastischer Vorteil 0ist, dass die
Open-Soure-Entwicklungsplattform [[http://www.github.com][GitHub]] Jekyll-Seiten kostenlos ohne
Werbung publizieren lässt. So ist diese Seite kostenfrei unter
[[http://knupfer.github.io]] gehostet. Dieser Service ist aber nicht
alles, GitHub kompiliert für einen sogar den Sourcecode der
Homepage. Jedoch gibt es hier auch kleine Einschränkungen, so werden
zusätzliche Plugins beispielsweise aus Sicherheitsgründen nicht
unterstützt.

*** Emacs
Emacs ist wohl einer der ältesten und umfangreichesten
Texteditoren. Dass ein Texteditor umfangreich oder gar komplex sein
kann, mag zunächst überraschen, jedoch kann Emacs äußerst erstaunlich
Dinge:
- Programmcode kann im Editor kompiliert werden
- Syntaxhighlighting
- Verschiedene Themes
- Sehr starke und umfangreiche Navigationswerkzeuge, z. B.:
  - Cursorbewegung durch Shortcuts
  - Löschen von Buchstaben, Wörtern oder ganzen Absätzen
  - Wechseln von verschiedenen Fenstern per Shortcut
  - Fenstersplitting usw.
- Durch seine Programmierbarkeit starke Erweiterbarkeit:
  - kann der Editor direkt als Taschenrechner verwendet werden
  - gibt viele IDE, die für Emacs von der Community programmiert
    wurden

**** Org Mode
[[http://www.orgmode.org][Org Mode]] ist eine von vielen möglichen Erweiterungen von Emacs, die
selbst so umfangreich ist, dass die Dokumentation ein sehr dickes Buch
füllt. Mit Org Mode kann man, wie der Name bereits sugeriert, sich
organisieren. Dies fängt von ganz simplen Aufgaben wie Todo-Listen an,
geht weiter zu einem sehr guten Outliner und auch zu mehrsprachigem
(Computersprachen) literateprogramming.

***** Exportfunktionen
Eine große Stärke von Org Mode sind die Exportfunktionen. Es
kann u. a. nach LaTeX, Html, Docbook und Ascii exportiert
werden. Dieses Feature ist so sehr ausgereift, dass die resultierenden
Dokumente oft sehr gut aussehen. Automatisch wird ein
Inhaltsverzeichnis generiert, ausgezeichnet, ggf. code compiliert
uvm. Aller Text dieser Homepage ist ausschließlich in Org Mode
verfasst, in ein Html exportiert und danach mit einem Shellscript den
Anforderungen von Jekyll angepasst.

***** Syntaxhighlighting
Der Export von Syntaxhighlighting von Html ist ein wenig
aufwendig. Man könnte zwar auf das Syntaxhighlighting von Jekyll
zurückgreifen, dies ist aber für helle Codeboxen optimiert, was nicht
meiner Designvorstellung entsprach. Eine Änderung dieses Highlightings
würde mit mühevollem ändern und testen jedes einzelnen
Codeschlüsselwortes in einer css-Datei einhergehen, worauf ich keine
Lust habe, da es auch möglich ist, dass diese Homepage ihr Äußeres
noch mehrfach verändert etc.

Mithilfe des Zusatzpakete htmlize.el für Emacs ist es mir gelungen,
Emacs so einzurichten, dass er automatisch das gleiche
Syntaxhighlighting, welches derzeit im Editor aktiv ist, in den
Html-export übernimmt. Über ein in Org Mode implementiertes Javascript
ist es sogar möglich, über links Referenzcodezeilen zu
manipulieren. Die Standardeinstellung war, die ganze Zeile gelb zu
färben, was sehr direkt war. Ich habe mich für eine dezente Version
entschieden und habe den Sourcecode so geändert, dass wenn der Cursor
über dem link ist, die entsprechende Codezeile fett dargestellt wird.

*** Das Shellscript
Da Jekyll einen speziellen Head einer Html-Datei benötigt, blieb mir
nichts anderes übrig, diesen Anfang über ein Shellscript in das durch
Emacs exportierte Html einzufügen. Ich habe Emacs so konfiguriert,
dass es nur den Body der Html exportiert und habe das Javascript für
das Syntaxhighlighting seperat abgespeichert. In dem Shellscript wird
der [[(head)][Head]] aus der .org-Datei flexibel extrahiert und in einer neuen
Datei abgespeichert, daraufhin das [[(java)][Javascript]] eingefügt und dann der
Html-Body. Zum Schluss wird noch die Datei umbenannt und in den von
Jekyll verabeiteten _posts-Ordner [[(move)][verschoben]]. Das Skript macht diese
Prozedur automatisch, [[(for)][mit allen .org-Dateien]], die exportiert wurden,
sprich die verändert wurden. Bereits aktuelle Posts werden nicht
modifiziert, so bleibt das Datum des Posts erhalten.

Hier zum nachlesen und nachdenken das Shellscript, natürlich mit
Syntaxhighlighting usw. Es wurden Gruppen von drei Bindestrichen durch
zwei Bindestriche eresetzt, um keine Probleme mit Jekyll zu bereiten.

#+BEGIN_SRC sh :exports code
#!/bin/sh

cd ~/git/knupfer.github.io/_org
rm ~/git/knupfer.github.io/_posts/*

TEMP='../_processing';
TOC='totalindex';

echo '--
layout: default
title: Index
--
<div id="table-of-contents2">
<h3>Index</h3>
<div id="text-table-of-contents2">
<ul>' > $TEMP/$TOC;

for DIRECTORY in */
do
    cd $DIRECTORY
    for FILE in *.org
    do
        DATEI=$(echo $FILE | sed 's_\(.*\).org_\1_');
        URL=$(echo /$DATEI | sed 's_-_/_; s_-_/_; s_-_/_; s_$_.html_')
        TITLE=$(sed -n '3,/--/ s_title: *"*\([^"]*\)"*_\1_p' $DATEI.org);
        FATHER=$(echo $DIRECTORY | sed 's:/::')
        
        test -e ../$TEMP/$DIRECTORY/$DATEI.html &&
        echo '<li><a href="'$URL'">'$TITLE'</a>' >> ../categorie.$FATHER &&
        sed -n '/<div id="text-table-of-contents">/,/<\/div>/p' ../$TEMP/$DIRECTORY/$DATEI.html | 
        tail -n +2 | 
        head -n -1 | 
        sed 's:\(href="\)#:\1'$URL'#:g' >> ../categorie.$FATHER &&
        sed -n '3,/--/ p' $DATEI.org | head --lines=-1 > ../$TEMP/$DIRECTORY/$DATEI.org.publish &&
        echo 'father: '$DIRECTORY | sed 's:/::' >> ../$TEMP/$DIRECTORY/$DATEI.org.publish &&
        echo '--' >> ../$TEMP/$DIRECTORY/$DATEI.org.publish &&
        sed 'N;
            s_[(</ul>)(</dl>)]\n</div>_&<p></p>_;
            P;
            s_file:///_/_;
            s_<h2>Table of Contents</h2>_<h3>Inhaltsverzeichnis</h3>_;
            D' ../$TEMP/$DIRECTORY/$DATEI.html >> ../$TEMP/$DIRECTORY/$DATEI.org.publish &&
        cat ../$TEMP/$DIRECTORY/$DATEI.org.publish > ../../_posts/$DATEI.html;
    done
    cd ..
done

for FILE in *.org
do
    DATEI=$(echo $FILE | sed 's_\(.*\).org_\1_');
    URL=$(echo /$DATEI | sed 's_-_/_; s_-_/_; s_-_/_; s_$_.html_')
    TITLE=$(sed -n '3,/--/ s_title: *"*\([^"]*\)"*_\1_p' $DATEI.org);
    FATHER=$(sed -n '3,/--/ s_father: *"*\([^"]*\)"*_\1_p' $DATEI.org);
    
    test -e $TEMP/$DATEI.html &&
    echo '<li><a href="'$URL'">'$TITLE'</a>' >> categorie.$FATHER &&
    sed -n '/<div id="text-table-of-contents">/,/<\/div>/p' $TEMP/$DATEI.html | 
    tail -n +2 | 
    head -n -1 | 
    sed 's:\(href="\)#:\1'$URL'#:g' >> categorie.$FATHER &&
    sed -n '3,/--/ p' $DATEI.org > $TEMP/$DATEI.org.publish &&
    sed 'N;
        s_[(</ul>)(</dl>)]\n</div>_&<p></p>_;
        P;
        s_file:///_/_;
        s_<h2>Table of Contents</h2>_<h3>Inhaltsverzeichnis</h3>_;
        D' $TEMP/$DATEI.html >> $TEMP/$DATEI.org.publish &&
    cat $TEMP/$DATEI.org.publish > ../_posts/$DATEI.html;
done



mv categorie. categorie.zzz

for INDEX in categorie.*
do
    if [ "$(echo $INDEX | sed 's_categorie\.\(.*\)_\1_')" != 'zzz' ]
    then
        echo '<li><a href="">' $(echo $INDEX | 
            sed 's_categorie\.\(.*\)_\1_') '</a>' '<ul>' >> $TEMP/$TOC;
    else
        echo '<li><a href="">' Verschiedenes '</a>' '<ul>' >> $TEMP/$TOC
    fi
    cat $INDEX >> $TEMP/$TOC;
    echo '</ul>' >> $TEMP/$TOC;
done

echo '</ul></div></div>' >> $TEMP/$TOC;
cp $TEMP/$TOC  ../$TOC.html;
mv categorie.* $TEMP;
mv ../_posts/about.html ../about.html;
test -e *.org~ && rm *.org~ ;
#+END_SRC

* Exportskript
[2014-07-30 Wed]
#+BEGIN_SRC emacs-lisp :exports both
(defun knu-split-org-articles (file-name)
  "Take the base of a .org file and extract all headings with timestamps in
different html files for jekyll."
  (with-temp-buffer
    (insert-file-contents file-name)
    (delete-file file-name)
    (delete-directory (concat (file-name-directory file-name) "_posts/") t)
    (mkdir (concat (file-name-directory file-name) "_posts/"))
    (let ((articles (split-string (buffer-string) ".*\n<h2 .*</span>")))
      (while articles
        (with-temp-buffer
          (insert (concat "---\nlayout: post\ntitle:" (pop articles)))
          (goto-char (point-min))
          (when (re-search-forward
                 "<div id=\"postamble\" class=\"status\">" nil t)
            (goto-char (match-beginning 0))
            (forward-line -1)
            (delete-region (point) (point-max)))
          (goto-char (point-max))
          (re-search-backward "</div>" nil t)
          (replace-match "" nil nil)
          (goto-char (point-min))
          (while (re-search-forward
                  "<span class=\"section-number[^/]*</span>" nil t)
            (replace-match ""))
          (goto-char (point-min))
          (while (re-search-forward "<img src=\"images/" nil t)
            (replace-match "<img src=\"/images/"))
          (goto-char (point-min))
          (when (re-search-forward "title: \\(.*\\)\\(</h2>\\)" nil t)
            (replace-match "\n---" nil nil nil 2)
            (let ((title (match-string 1)))

              (when (re-search-forward
                     ".*<span class=\"timestamp\">\\[\\([0-9-]*\\).*" nil t)
                (let ((date (match-string 1)))
                  (replace-match "")
                  (write-file (concat (file-name-directory file-name) "_posts/"
                                      date
                                      "-" (md5 title) ".html")))))))))))
(run-with-idle-timer
 0.5 nil 'knu-split-org-articles
 (concat (file-name-sans-extension (buffer-file-name)) ".html"))
nil
#+END_SRC
