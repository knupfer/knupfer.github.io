#+BEGIN_COMMENT
---
layout: post
title: Adaptives Testen
father: Wissenschaft
---
#+END_COMMENT
* Adaptives Testen
Beim adaptiven Testen werden die Möglichkeiten einer computerbasierten Testung in erweitertem Maße ausgenutzt.
Es wird innerhalb der Testung auf Grund der bereits beantworteten Fragen auf den Fähigkeitsgrad des Probanden geschätzt,
um ihm als nächstes eine Frage zu geben, die diesen reflektiert.

#+BEGIN_SRC ditaa :file /images/adaptiveditaa.png :exports code

+---------+   +-----------------+   +---------------+
| Item    |-->| Modellschätzung |-->|   Schätzung   |
| Antwort |   +-----------------+   | nächstes Item |
+---------+                         +---------------+
    ^                                       |
    |                                       |
    +---------------------------------------+

#+END_SRC 

** Kriterien
Wie genau die Frage ausgewählt wird hängt vornehmlich von den Zielen der Testung und den Nebenwirkungen, die man ggf. 
vermeiden möchte, ab. So wäre eine Frage, die der Proband mit einer Chance von 50% lösen kann ideal im Sinne eines 
Informationsgewinnes über den Probanden, da die Entropie maximal ist. 

Entropie stellt nicht nur Chaos, sondern auch Informationsdichte nach Shannon dar 
(vgl. [[http://de.wikipedia.org/wiki/Entropie_%28Informationstheorie%29][Wikipedia]]).
Die folgende Formel ergiebt die Entropie eines Ereignisses. Hierbei ist $n$ die Anzahl der Möglichkeiten und $p_i$ 
die jeweiligen Wahrscheinlichkeiten dieser (sich ausschließender) Möglichkeiten.

$$ H = - \sum_{i=1}^n p_i \cdot \log_2{p_i} $$

Betrachten wir ein binäres System, also nur Fragen die entweder vollständig falsch oder vollständig richtig beantwortet
werden können, so haben wir $n = 2$ und $p_2 = 1 - p_1$.

$$ H_2 = - p \cdot \log_2{p} - (1 - p) \cdot \log_2(1 - p) $$

Hieraus ergiebt sich, dass der Informationsgewinn bei $p = .5$ (im binären) maximal ist, da die Funktion symmetrisch ist und bei
$p = 0$ auf beiden Seiten $0$ ergibt.

$$ H_{max} = - 0.5 \cdot \log_2{0.5} - 0.5 \cdot \log_2{0.5} = 1 \mathrm{bit} $$

Hierbei ist $bit$ die übliche Einheit des Informationsgehaltes, auf Grund des $\log_2$. Es kann also ein Informationsgehalt
von 8 bit mit Hilfe eines bytes dargestellt werden, sprich mit 8 Nullen oder Einsen (z.B. 10101010).

#+BEGIN_SRC R :results output graphics :file /images/entropie.png :exports results
x = (0:100)/100
y = -x*log(x,2)-(1-x)*log(1-x,2)
plot(x,y,type="l",xlab=expression(Lösungswahrscheinlichkeit),ylab=expression("Entropie in bit"),  main="Entropieverteilung")
#+END_SRC

#+RESULTS:
[[file:/images/entropie.png]]

Mit der Formel für tatsächliche und für maximale Entropie kann die Redundanz ausgerechnet werden, welche in einer 
Testkonstruktion als Indikator für das Potential der Verbesserung durch ein adaptives Testverfahren benutzt werden kann.

$$ R = H_{max} - H $$

Somit kann im binären ein Test im idealfall um $R$ Fragen verkleinert werden, ohne an Informationen einzubüßen. 
Befinden sich im Test auch Fragen, die nicht binär sind, so verändert sich die Situation ein wenig.
Die Entropie wird stets maximal bei gleichen Wahrscheinlichkeiten. Gibt es nun bei einer Frage z.B. 0, 1, 2 oder 3 Punkte
gibt es eine maximale Entropie von $H_{max} = - \log_2{0.25} = 2 \mathrm{bit}$. Somit kann man einen Test auch um $R/2$ solcher
Fragen ohne Informationsverlust verkleinern.

*** Informationsgehalt in KoMus
Im August diesen Jahres habe ich mit Teilen des KoMus-Testes für musikalische Kompetenz eine empirische Studie
durchgeführt. Der KoMus-Test liegt in einem nicht adaptiven Format vor.

Interessant ist nun, die Überlegung, wieviel der Test von einer Überführung in einen adaptiven Test profitieren würde.

**** Durchschnittlicher Schüler
Der Simplizität halber können wir annehmen, dass ein Schüler genau die durchschnittlichen Lösungswahrscheinlichkeiten
für ein Item aufweist.

So muss man nur die Entropie mit den klassischen Itemschwierigkeiten der Items berechnen:

$$ H = \sum_{i=1}^n(- P_i \cdot \log_2{P_i} - (1 - P_i) \cdot \log_2(1 - P_i)) $$

Hierbei ist $P_i$ die klassische Itemschwierigkeit des Items $i$ und $n$ die Anzahl der Items. Ferner nimmt diese Formel
nur dichotome Items an.

**** Vierdimensional
Die Entropie ist jedoch eigentlich noch niedriger, wenn man die Schwierigkeiten adaptiv berechnet. Es handelt sich aber
immernoch um einen nicht adaptiven Test mit fester Reihenfolge. Es wird nur eine spezialform der Entropie, die bedingte
Entropie, benutzt (vgl. [[http://de.wikipedia.org/wiki/Bedingte_Entropie][Wikipedia]]).

Um die bedingte Lösungschance und somit auch die bedingte Entropie zu berechnen, werden alle Items der selben Dimension
einer logistisch binären Regression verwendet um die individuell Itemschwierigkeit eines Items der gleichen Dimension zu
berechnen.

$$ H = \sum_{D=1}^4(\sum_{i=1}^{n(D)}(- R \cdot \log_2{R}) - (1 - R) \cdot \log_2(1- R))$$
$$ R = Reg_2(P_{ivD}|\sum_{m=1}^{i(D)-1}P_{vmD}) $$

Hierbei ist $Reg_2(a|b)$ die binärlogistische Regression mit der AV $a$ und den UV $b$.

**** n-Dimensional
Das obige Modell nimmt jedoch an, dass die einzelnen Dimensionen nicht korrelieren und somit Itemantworten einer Dimension 
keine Information (also Entropiesenkung) über andere Dimensionen zulassen.

Dies ist aber eine nicht notwendige Einschränkung, die die Berechnung nicht nur weniger effektiv, sondern auch
komplizierter macht.

Berücksichtigen wir unabhängig von der Dimensionszugehörigkeit einfach alle bereits beantworteten Items, die einen
signifikanten Einfluss auf die Frage haben, haben wir im Endeffekt ein n-dimensionales Modell, wobei $n$ die Anzahl der
Fragen ist.

$$ H = \sum_{i=1}^n(- R \cdot \log_2{R} - (1 - R) \cdot \log_2(1 - R)) $$
$$ R = Reg_2(P_{iv}|\sum_{m=1}^{i-1}P_{vm}) $$

***** Probleme
Mögliche Probleme dieser Methode sind fehlende Datensätze, da die binärlogistische Regression normalerweise alle Fälle
ausschließt, die auch nur bei einem der UV keinen Wert hat. Durch den maßgeschneiderten Testweg ist es bei einem 
Itempool, der nicht komplett erschöpft wird, unmöglich diese Regression so durchzuführen. Dementsprechend müssen
andere Methoden gewählt werden, um mit fehlenden Daten umzugehen.

Eine Möglichkeit wäre, nicht vorhandene Antworten in den Datensätzen zu simulieren. Diese Simulation würde von den Items
die am sichersten geschätzt werden können zu den Items, die schwer geschätzt werden können stattfinden.

Der Grund hierfür liegt daran, dass bei einer sehr sicheren Schätzung der Entropiegehalt sich nur wenig ändert, 
aber es gleich viel mehr Personen gibt, die seriös für schwerere Schätzungen verwendet werden können, was diese
Schätzungen erleichtert.

Die Simulation sollte nach jeder Testung durchgeführt werden, um die Simulation mehr und mehr zu verbessern. Das dieses
Verfahren direkt einen Nutzen erbringt, sollte daran sichtbar sein, dass die Lösungen der alten Datensätze immer besser
retrospektiv vorhergesagt werden können und somit davon außgegangen werden kann, dass auch bei aktuellen Testungen
die Schätzungen besser sind und somit effektiver gemessen werden kann.

* Nicht adaptives Testen
 
* Adaptives Testen

** Umsetzung

*** Programmierung

**** Initialisierung
#+NAME: statistic
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
library(multicore)
library(foreach)
library(doMC)
registerDoMC(8)
calculationtime <- proc.time()
komus = read.table("data/data_komus_bin2.dat",header=TRUE) 
Entropie = NULL
chances = NULL
fitting = NULL
modell = NULL
summe = data.frame()
############
#items = length(komus)
#persons = length(komus[,1])
items = 50
persons = 5
############

EEE = data.frame(matrix(ncol = 1, nrow = items))
SumSD = data.frame(matrix(ncol = 1, nrow = items))
names(EEE) = 'kill'
names(SumSD) = 'kill'

#+END_SRC

#+RESULTS: statistic

**** Unbedingte und bedingte Entropie in normaler Reihenfolge
#+NAME: statistic1
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
############## Bedingte und undbedingte Entropie in normaler Reihenfolge
modell[[1]] = glm(V1 ~ 1, data = komus, family = "binomial"(link=logit))

for (i in 2:items) {
    modell[[i]] = glm(reformulate(names(komus[1:i-1]), names(komus[i])), data = komus, family = "binomial"(link=logit))
}
fitting = modell
<<fitting>>
chances = simplify2array(mclapply(fitting, predict, type="response"))
Entropietemp = -chances*log(chances,2)-(1-chances)*log(1-chances,2)

SumSDtemp = sd(Entropietemp[,1])
for (i in 2:length(Entropietemp[1,])) {
SumSDtemp[i] = sd(rowSums(Entropietemp[,1:i]))
}

SumSD$bedunsort = SumSDtemp 

EEE$bedunsort = colMeans(Entropietemp)
EEE$unbedunsort = -colMeans(komus[1:items])*log(colMeans(komus[1:items]),2)-(1-colMeans(komus[1:items]))*log(1-colMeans(komus[1:items]),2)
EEE$unbedsort = sort(EEE$unbedunsort, decreasing = TRUE)


#+END_SRC

#+RESULTS: statistic1

**** Bedingte, sortierte Entropie
#+NAME: statistic2
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
############### Bedingte Entropie in über den Durchschnitt sortierten Reihenfolge
komus2 = komus[c(names(sort(-colMeans(komus[1:items])*log(colMeans(komus[1:items]),2)-(1-colMeans(komus[1:items]))*log(1-colMeans(komus[1:items]),2), decreasing = TRUE)))]
modell[[1]] = glm(V1 ~ 1, data = komus2, family = "binomial"(link=logit))

for (i in 2:items) {
    modell[[i]] = glm(reformulate(names(komus2[1:i-1]), names(komus2[i])), data = komus2, family = "binomial"(link=logit))
}
fitting = modell
<<fitting>>
chances = simplify2array(mclapply(fitting, predict, type="response"))
Entropietemp = -chances*log(chances,2)-(1-chances)*log(1-chances,2)

SumSDtemp = sd(Entropietemp[,1])
for (i in 2:length(Entropietemp[1,])) {
SumSDtemp[i] = sd(rowSums(Entropietemp[,1:i]))
}

SumSD$sortbed = SumSDtemp 

EEE$sortbed = colMeans(Entropietemp)

#+END_SRC

#+RESULTS: statistic2

**** Durchschnittlich bedingtsortierte Entropie
#+NAME: statistic3
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
####### Bedingte Entropie, sortiert nach jeder durchschnittlichen Antwort
fragen = c(44)

modell[[1]] = glm(V44 ~ 1, data = komus, family = "binomial"(link=logit))

for (i in 2:items) {
    Entropietemp = NULL
    fitting = NULL

    for (j in 1:length(komus[-fragen])) {
        fitting[[j]] = glm(reformulate(names(komus[fragen]), names(komus[-fragen][j])), data = komus, family = "binomial"(link=logit))
    }

    <<fitting>>
    chances = simplify2array(mclapply(fitting, predict, type="response"))
    Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2))
    fragen = c(fragen, which(names(komus[-fragen][which(colMeans(Entropietemp) == max(colMeans(Entropietemp)))]) == names(komus)))
    modell[[i]] = fitting[[which(colMeans(Entropietemp) == max(colMeans(Entropietemp)))]]
}

chances = simplify2array(mclapply(modell, predict, type="response"))
Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2))

SumSDtemp = sd(Entropietemp[,1])
for (i in 2:length(Entropietemp[1,])) {
SumSDtemp[i] = sd(rowSums(Entropietemp[,1:i]))
}

SumSD$durchschbedsort = SumSDtemp 
EEE$durchschbedsort = colMeans(Entropietemp)


#+END_SRC

#+RESULTS: statistic3

**** Individuellbedingtsortierte Entropie
#+NAME: statistic4
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
####### Bedingte Entropie, sortiert nach jeder individuellen Antwort
Entropieall = NULL
chances = NULL
modell[[1]] = glm(V44 ~ 1, data = komus, family = "binomial"(link=logit))

Entropieall = simplify2array(foreach(k=1:persons) %dopar% {
    fragen = c(44)
    for (i in 2:items) {
        Entropietemp = NULL
        fitting = NULL

        for (j in 1:length(komus[-fragen])) {
            fitting[[j]] = glm(reformulate(names(komus[fragen]), names(komus[-fragen][j])), data = komus, family = "binomial"(link=logit))
        }
        <<fitting>>
        chances = simplify2array(mclapply(fitting, predict, komus[k,], type="response"))
        Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2))
        fragen = c(fragen, which(names(komus[-fragen][which((Entropietemp) == max((Entropietemp)))]) == names(komus)))
        modell[[i]] = fitting[[which((Entropietemp) == max((Entropietemp)))]]
    }

    chances = simplify2array(mclapply(modell, predict, komus[k,], type="response"))
    Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2))
    return(Entropietemp)
})

SumSDtemp = sd(Entropieall[1,])
for (i in 2:length(Entropieall[,1])) {
SumSDtemp[i] = sd(colSums(Entropieall[1:i,]))
}

SumSD$indivbedsort = SumSDtemp 
EEE$indivbedsort = rowMeans(Entropieall)

#+END_SRC

#+RESULTS: statistic4

**** Individuellbedingtsortierte Entropie mit Trennschärfe
#+NAME: statistic5
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
####### Bedingte Entropie, sortiert nach jeder individuellen Antwort
Entropieall = NULL
chances = NULL
beta = NULL

if (!exists("information")) {
    information = simplify2array(foreach(m=1:length(komus)) %dopar% {
        for (n in 1:(length(komus)-1)) {
            beta[[n]] = glm(reformulate(names(komus[m]), names(komus[-m][n])), data = komus, family = "binomial"(link=logit))
        }
        chances = simplify2array(lapply(beta, predict, type="response"))
        chancetemp = unlist(lapply(komus[m],mean))
        Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2))
        information = sum(colMeans(Entropietemp)) + (-chancetemp*log(chancetemp,2)-(1-chancetemp)*log(1-chancetemp,2))
        return(information)
    })
    information = -(information - sum(-colMeans(komus)*log(colMeans(komus),2)-(1-colMeans(komus))*log(1-colMeans(komus),2)))
}

modell[[1]] = glm(V44 ~ 1, data = komus, family = "binomial"(link=logit))

Entropieall = simplify2array(foreach(k=1:persons) %dopar% {
    fragen = c(44)
    for (i in 2:items) {
        Entropietemp = NULL
        fitting = NULL
        for (j in 1:length(komus[-fragen])) {
            fitting[[j]] = glm(reformulate(names(komus[fragen]), names(komus[-fragen][j])), data = komus, family = "binomial"(link=logit))
        }

        <<fitting>>
        ## TODO stimmt das so?
        chances = simplify2array(lapply(fitting, predict, komus[k,], type="response"))
    
        Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2)) + (information[-fragen]*(1 - (length(fragen)+1)/items))
        fragen = c(fragen, which(names(komus[-fragen][which((Entropietemp) == max((Entropietemp)))]) == names(komus)))
        modell[[i]] = fitting[[which((Entropietemp) == max((Entropietemp)))]]

    }

    chances = simplify2array(lapply(modell, predict, komus[k,], type="response"))
    Entropietemp = (-chances*log(chances,2)-(1-chances)*log(1-chances,2))
    
    return(Entropietemp)
})


SumSDtemp = sd(Entropieall[1,])
for (i in 2:length(Entropieall[,1])) {
SumSDtemp[i] = sd(colSums(Entropieall[1:i,]))
}

SumSD$indivbedsorttrenn = SumSDtemp 
EEE$indivbedsorttrenn = rowMeans(Entropieall)

#+END_SRC

#+RESULTS: statistic5

**** Experimenteller Code
#+BEGIN_SRC R :session stat :exports code :results output :noweb yes
calculationtime <- proc.time()




#+END_SRC

#+RESULTS:
: [1] 0.2655087

**** Schlussberechnungen
#+NAME: statisticend
#+BEGIN_SRC R :session stat :exports both :results output :noweb yes
################# Schlussberechnungen

if (names(EEE[1]) == 'kill') {
    EEE = EEE[-1]
}
if (names(SumSD[1]) == 'kill') {
    SumSD = SumSD[-1]
}

for (i in 1:length(EEE[1,])) {
    for (j in 1:length(EEE[,1])) {
        summe[j,i] = sum(EEE[1:j,i])
    }
}
names(summe) = names(EEE)

if (exists("benchmark")) {
    benchmark = array(c(benchmark,(proc.time() - calculationtime)[3]))
} else {
    benchmark = (proc.time() - calculationtime)[3]
}

benchmark
#+END_SRC

#+RESULTS: statisticend
#+begin_example
   durchschbedsort   sortbed sortbedsort bedunsort unbedunsort unbedsort
1        0.9998228 0.8125179   0.9974078 0.8125179   0.8125179 0.9984045
2        1.9966436 1.8099257   1.9777564 1.6949192   1.6987334 1.9958440
3        2.9903563 2.7902743   2.9523106 2.6042080   2.6443937 2.9920908
4        3.9793009 3.7648285   3.8636329 3.4742341   3.6019472 3.9852679
5        4.9609520 4.5187020   4.7618910 4.4402305   4.5981941 4.9708654
6        5.9265612 5.4300243   5.6158164 5.2858708   5.5837916 5.9508607
7        6.8899568 6.3282823   6.4283343 6.1541337   6.5812311 6.9084142
8        7.8491889 7.1822078   7.2201542 7.1017005   7.5796355 7.8540745
9        8.8014315 7.9594998   7.9974462 7.9913882   8.5596309 8.7402900
10       9.7529300 8.7513197   8.7513197 8.9370433   9.5528079 9.5528079
     bedsort indivbedsort indivbedsorttrenn
1  0.9659964    0.9998228         0.9998228
2  1.9135632    1.9995006         1.9401171
3  2.8592183    2.9992296         2.9236741
4  3.7685070    3.9988626         3.8833395
5  4.6581947    4.9970906         4.8606868
6  5.5405960    5.9959740         5.8601034
7  6.4106222    6.9959323         6.8542033
8  7.2788851    7.9958163         7.8308658
9  8.1245254    8.9944425         8.8161971
10 8.9370433    9.9929196         9.8154871
 [1]   21.833  304.617   53.133  656.001  686.015  692.971  700.081  782.657
 [9]  790.530  799.315  801.223  807.715  816.267  819.084  827.460  835.891
[17]  844.098  846.367  901.568  917.687  936.795  942.953  981.047  999.155
[25] 1002.845 1007.719 1034.616 1079.935 1124.803 1192.753 1195.413 1209.383
[33] 1231.030 1271.202 1281.525 1283.636 1321.782 1325.576 1331.860 1333.447
[41] 1334.757 1336.929 1341.149 1346.157 1352.009   40.518   62.549   66.593
[49]   73.427  159.076  162.448  283.959  309.399  312.934  321.920  407.877
[57]  496.741  519.168  538.641  543.335  561.263  636.384  900.850  917.018
[65]  929.047  933.375  946.377  953.158  972.639  991.263  997.769 1008.808
[73] 1011.829 1028.920 1063.926 1067.599 1075.784 1106.952 1211.732 1228.636
[81] 1279.492 1295.312 1321.068 1324.841 1326.975 1423.645 2371.376  301.476
#+end_example

**** Formel für die Modellanpassung
#+NAME: fitting
#+BEGIN_SRC R
#fitting = mclapply(fitting, step, trace = 0)
#fitting = mclapply(fitting, step, ~.^2, trace = 0)
#+END_SRC

**** Benchmark
#+BEGIN_SRC R :session stat :noweb yes :results output graphics :file /images/benchmark.png :exports both
plot(benchmark, type="l", col=rgb(0,0,0), ann=F)
title(xlab="Durchlauf")
title(ylab="Dauer")
#+END_SRC

#+RESULTS:
[[file:/images/benchmark.png]]

**** Entropiegrafik
#+NAME: grafik
#+BEGIN_SRC R :session stat :noweb yes :results output graphics :file /images/entropie2.png :exports both

farbe = NULL
for (j in 1:(length(summe[1,]))) {
    farbe[j] = rgb(runif(1, 0, 1), runif(1, 0, 1), runif(1, 0, 1))
}

plot(1:length(summe[,1]), type="l", col=rgb(0,0,0), ann=F)
for (i in 1:(length(summe[1,]))) {
    lines(summe[,i], col=farbe[i])
    if (dim(SumSD[names(SumSD) == names(summe[i])])[2] != 0) {
        lines(summe[,i]+SumSD[names(summe[i])],lty = 4, col=farbe[i])
        lines(summe[,i]-SumSD[names(summe[i])],lty = 4, col=farbe[i])
    }
}

title(xlab="Anzahl der beantworteten Fragen")
title(ylab="Entropie in bit")
legend(1, length(summe[,1]), c(names(summe), round(benchmark[length(benchmark)])), cex=0.9, col=c(farbe, rgb(1,1,1)), lty=1);

#+END_SRC

#+RESULTS: grafik
[[file:/images/entropie2.png]]
